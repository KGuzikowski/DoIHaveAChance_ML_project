{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation: \n",
    "# Some data is missing (but we know what it should be) or some data should be changed.\n",
    "df = pd.read_csv('./speed_dating_data.csv', encoding = 'ISO-8859-1')\n",
    "\n",
    "def fooCareer(col):\n",
    "    selected_df = df[col]\n",
    "    career_c = {\n",
    "        'lawyer': 1.,\n",
    "        'law': 1.,\n",
    "        'Economics': 7.,\n",
    "        'tech professional': 5.\n",
    "    }\n",
    "    \n",
    "    for x, row in selected_df.iterrows():\n",
    "            career = df.at[x, col[0]]\n",
    "            if career in career_c:\n",
    "                df.at[x, col[1]] = career_c[career]\n",
    "                \n",
    "\n",
    "def fooGoal(col):\n",
    "    selected_df = df[col]\n",
    "    goal_c = {\n",
    "        1: 1,\n",
    "        2: 1,\n",
    "        3: 1,\n",
    "        4: 0,\n",
    "        5: 1,\n",
    "        6: 1,\n",
    "    }\n",
    "    \n",
    "    for x, row in selected_df.iteritems():\n",
    "        goal = df.at[x, col]\n",
    "        if goal in goal_c:\n",
    "            df.at[x, col] = goal_c[goal]\n",
    "\n",
    "fooCareer(['career','career_c'])\n",
    "fooGoal('goal')\n",
    "df.to_csv('base_dating_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-023d9bb5a582>:9: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  multiplier = 100/row.sum()\n",
      "<ipython-input-14-023d9bb5a582>:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  df.at[x, col] *= multiplier\n"
     ]
    }
   ],
   "source": [
    "# data is inconsistent:\n",
    "# Some values (itâ€™s based on waves) are 1-10 and others are 1-100 so\n",
    "# they should be normalized so that all are 1-100.\n",
    "df = pd.read_csv('base_dating_data.csv')\n",
    "\n",
    "def func(lista):\n",
    "    selected_df = df[lista]\n",
    "    for x, row in selected_df.iterrows():\n",
    "        multiplier = 100/row.sum()\n",
    "        for col in lista:\n",
    "            df.at[x, col] *= multiplier\n",
    "\n",
    "lista = ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']\n",
    "lista1 = ['attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o']\n",
    "lista3 = ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']\n",
    "lista4 = ['attr1_2', 'sinc1_2', 'intel1_2', 'fun1_2', 'amb1_2', 'shar1_2']\n",
    "\n",
    "func(lista)\n",
    "func(lista1)\n",
    "func(lista3)\n",
    "func(lista4)\n",
    "\n",
    "df.to_csv('base_dating_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column met_o had values 1/2. It needs to be changed to 0/1.\n",
    "df = pd.read_csv('./base_dating_data.csv')\n",
    "df['met_o'] = df['met_o'].apply(lambda x: x - 1)\n",
    "df.to_csv('base_dating_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed some rows (by hand) because they didn't have `pid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below you can see code that prepares data for the basic matrix factorization.\n",
    "Here in the base matrix we only have information about match. So we need a set of vectors, where each vector describes each date (holds both ids and information about match).\n",
    "\n",
    "Task will requite two base matrices.\n",
    "1. Matrix where men are \"users\" and women are \"products\". It will then be used to recommend women to men because matrix will say what's the predicted rating of a woman in eyes of man. Basically it will answer the question: **\"How likely is that a man will like a woman?\"**. Let's call this matrix/data frame **\"men_like_women\"**.\n",
    "1. Matrix where women are \"users\" and men are \"products\". It will then be used to recommend men to women because matrix will say what's the predicted rating of a man in eyes of woman. Basically it will answer the question: **\"How likely is that a woman will like a man?\"**. Let's call this matrix/data frame **\"women_like_men\"**.\n",
    "\n",
    "Why such analogies? It may help to understand how do this human relations task translates into recommender systems world.\n",
    "\n",
    "These matrices will be used to train models (with matrix factorization) which will then be saved into csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "men_like_women_df:\n",
      "         id    pid  decision\n",
      "0      11.0    1.0       0.0\n",
      "1      11.0    2.0       0.0\n",
      "2      11.0    3.0       0.0\n",
      "3      11.0    4.0       0.0\n",
      "4      11.0    5.0       0.0\n",
      "...     ...    ...       ...\n",
      "4179  552.0  526.0       0.0\n",
      "4180  552.0  527.0       0.0\n",
      "4181  552.0  528.0       0.0\n",
      "4182  552.0  529.0       0.0\n",
      "4183  552.0  530.0       0.0\n",
      "\n",
      "[4184 rows x 3 columns]\n",
      "\n",
      "women_like_men_df:\n",
      "         id    pid  decision\n",
      "0       1.0   11.0       1.0\n",
      "1       1.0   12.0       1.0\n",
      "2       1.0   13.0       1.0\n",
      "3       1.0   14.0       1.0\n",
      "4       1.0   15.0       1.0\n",
      "...     ...    ...       ...\n",
      "4179  530.0  548.0       0.0\n",
      "4180  530.0  549.0       1.0\n",
      "4181  530.0  550.0       0.0\n",
      "4182  530.0  551.0       0.0\n",
      "4183  530.0  552.0       1.0\n",
      "\n",
      "[4184 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split into vectors, let's have two matrices as describes above.\n",
    "base_df = pd.read_csv('./data_for_matrix.csv')\n",
    "men_like_women_data = []\n",
    "women_like_men_data = []\n",
    "\n",
    "for _, row in base_df.iterrows():\n",
    "    vector = {\n",
    "        'id': row['iid'],\n",
    "        'pid': row['pid'],\n",
    "        'decision': row['dec'],\n",
    "    }\n",
    "    if row['gender'] == 0:\n",
    "        # it's a woman\n",
    "        women_like_men_data.append(vector)\n",
    "    else:\n",
    "        men_like_women_data.append(vector)\n",
    "\n",
    "men_like_women_df = pd.DataFrame(men_like_women_data)\n",
    "women_like_men_df = pd.DataFrame(women_like_men_data)\n",
    "\n",
    "print(\"men_like_women_df:\")\n",
    "print(men_like_women_df)\n",
    "print(\"\\nwomen_like_men_df:\")\n",
    "print(women_like_men_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare train and test data sets\n",
    "\n",
    "### Make ids contiguous\n",
    "We need to change the data. As you can see ids are not contiguous and they must be. So let's change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_col(column):\n",
    "    \"\"\"Encodes a pandas column with continous ids.\"\"\"\n",
    "    unique = column.unique()\n",
    "    old_to_new = {o: i for i, o in enumerate(unique)}\n",
    "    return np.array([old_to_new.get(x, -1) for x in column])\n",
    "\n",
    "\n",
    "def encode_data(df):\n",
    "    \"\"\"Encodes data with continous person and partner ids.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col_name in [\"id\", \"pid\"]:\n",
    "        col = proc_col(df[col_name])\n",
    "        df[col_name] = col\n",
    "        df = df[df[col_name] >= 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now use these functions to make ids contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "         id    pid  decision\n",
      "0      11.0    1.0       0.0\n",
      "1      11.0    2.0       0.0\n",
      "2      11.0    3.0       0.0\n",
      "3      11.0    4.0       0.0\n",
      "4      11.0    5.0       0.0\n",
      "...     ...    ...       ...\n",
      "4179  552.0  526.0       0.0\n",
      "4180  552.0  527.0       0.0\n",
      "4181  552.0  528.0       0.0\n",
      "4182  552.0  529.0       0.0\n",
      "4183  552.0  530.0       0.0\n",
      "\n",
      "[4184 rows x 3 columns]\n",
      "\n",
      "After:\n",
      "       id  pid  decision\n",
      "0       0    0       0.0\n",
      "1       0    1       0.0\n",
      "2       0    2       0.0\n",
      "3       0    3       0.0\n",
      "4       0    4       0.0\n",
      "...   ...  ...       ...\n",
      "4179  276  269       0.0\n",
      "4180  276  270       0.0\n",
      "4181  276  271       0.0\n",
      "4182  276  272       0.0\n",
      "4183  276  273       0.0\n",
      "\n",
      "[4184 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print('Before:')\n",
    "print(men_like_women_df)\n",
    "men_like_women_df = encode_data(men_like_women_df)\n",
    "print('\\nAfter:')\n",
    "print(men_like_women_df)\n",
    "\n",
    "women_like_men_df = encode_data(women_like_men_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split into train and test data sets\n",
    "Standard sklearn function `train_test_split` doesn't do a job here because both test and train data sets should include all people. So after using `train_test_split` we need to transition some people between sets to ensure that both sets are correct. The same goes for partners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "def train_test_split(df, test_size=0.2):\n",
    "    # Use standard train_test_split.\n",
    "    df_train, df_test = tts(df, test_size=test_size)\n",
    "    no_train_unique = len(df_train.id.unique())\n",
    "    no_test_unique = len(df_test.id.unique())\n",
    "    \n",
    "    # See what people are missing in the test set.\n",
    "    diff = np.setdiff1d(df_train.id.unique(), df_test.id.unique())\n",
    "    for id in diff:\n",
    "        # For every missing person we need to exchange them for a\n",
    "        # different one so that sets still have the same number of elements.\n",
    "        person_to_send_to_test = df_train.loc[df_train.id == id].iloc[0]\n",
    "        person_to_send_to_train = None\n",
    "        # Choose some person from the test set to send to the train set.\n",
    "        ids = df_test.id.unique()\n",
    "        np.random.shuffle(ids)\n",
    "        for test_id in ids:\n",
    "            person = df_test.loc[df_test.id == test_id]\n",
    "            if len(person.index) > 1:\n",
    "                person_to_send_to_train = person.iloc[0]\n",
    "                break\n",
    "        if person_to_send_to_train is not None:\n",
    "            # Remove people that transit from old sets.\n",
    "            # .name holds the id of that row in the oryginal df.\n",
    "            df_train = df_train.drop(person_to_send_to_test.name)\n",
    "            df_test = df_test.drop(person_to_send_to_train.name)\n",
    "            # Add new people to sets.\n",
    "            df_train = pd.concat([df_train, person_to_send_to_train.to_frame().T], ignore_index=True)\n",
    "            df_test = pd.concat([df_test, person_to_send_to_test.to_frame().T], ignore_index=True)\n",
    "        else:\n",
    "            raise Exception(\"Couldn't find any person from people to send from the test to the train.\")\n",
    "        \n",
    "    # See what partners are missing in the test set.\n",
    "    diff = np.setdiff1d(df_train.pid.unique(), df_test.pid.unique())\n",
    "    for pid in diff:\n",
    "        # For every missing partner we need to exchange them for a\n",
    "        # different one so that sets still have the same number of elements.\n",
    "        partner_to_send_to_test = df_train.loc[df_train.pid == pid].iloc[0]\n",
    "        partner_to_send_to_train = None\n",
    "        \n",
    "        # Choose some partner from the test set to send to the train set.\n",
    "        # Need to make sure that both sets will still have all the people.\n",
    "        pids = df_test.pid.unique()\n",
    "        np.random.shuffle(pids)\n",
    "        for test_pid in pids:\n",
    "            partner = df_test.loc[df_test.pid == test_pid]\n",
    "            if len(partner.index) > 1:\n",
    "                # Make sure we don't remove a person completely.\n",
    "                id = partner.iloc[0].id\n",
    "                person_qty = len(df_test.loc[df_test.id == id].index)\n",
    "                if person_qty > 1:\n",
    "                    partner_to_send_to_train = partner.iloc[0]\n",
    "                    break\n",
    "        if partner_to_send_to_train is not None:\n",
    "            # Remove partners that transit from old sets.\n",
    "            df_train = df_train.drop(partner_to_send_to_test.name)\n",
    "            df_test = df_test.drop(partner_to_send_to_train.name)\n",
    "            # Add new people to sets.\n",
    "            df_train = pd.concat([df_train, partner_to_send_to_train.to_frame().T], ignore_index=True)\n",
    "            df_test = pd.concat([df_test, partner_to_send_to_test.to_frame().T], ignore_index=True)\n",
    "        else:\n",
    "            raise Exception(\"Couldn't find any partner from partners to send from the test to the train.\")\n",
    "                    \n",
    "    df_train = df_train.sort_values(by='id')\n",
    "    df_test = df_test.sort_values(by='id')\n",
    "    return df_train.reset_index(drop=True), df_test.sort_values(by='id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now just get correct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_like_women_train_df, men_like_women_test_df = train_test_split(men_like_women_df, test_size=0.2)\n",
    "men_like_women_no_men = len(men_like_women_train_df.id.unique())\n",
    "men_like_women_no_women = len(men_like_women_train_df.pid.unique())\n",
    "\n",
    "women_like_men_train_df, women_like_men_test_df = train_test_split(women_like_men_df, test_size=0.2)\n",
    "women_like_men_no_women = len(women_like_men_train_df.id.unique())\n",
    "women_like_men_no_men = len(women_like_men_train_df.pid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create matrix factorization models\n",
    "\n",
    "We will create and train several base MF (matrix factorization) models and for each of them let's do a cross validation to learn the best hyperparameters and parameters. Then we will compare the results and choose the best model.\n",
    "\n",
    "Some general explanations for models:\n",
    "* Models are train on only one batch because our data set is rather small.\n",
    "\n",
    "Good reading resource: https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# dev = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix factorization without bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with without xavier weights are:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.5302, 0.6026, 0.8964],\n",
      "        [0.3247, 0.0352, 0.9754],\n",
      "        [0.3641, 0.2975, 0.0356],\n",
      "        [0.5595, 0.6452, 0.6103],\n",
      "        [0.8939, 0.3568, 0.8017],\n",
      "        [0.3359, 0.9695, 0.5807],\n",
      "        [0.9689, 0.8340, 0.4982],\n",
      "        [0.2324, 0.6497, 0.2389],\n",
      "        [0.1100, 0.7207, 0.5310],\n",
      "        [0.2682, 0.9029, 0.8055]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.4752, 0.2166, 0.9902],\n",
      "        [0.0260, 0.4374, 0.2518],\n",
      "        [0.9437, 0.0936, 0.4996],\n",
      "        [0.9822, 0.1600, 0.5873],\n",
      "        [0.6288, 0.2265, 0.4612],\n",
      "        [0.6065, 0.6852, 0.9511],\n",
      "        [0.0112, 0.4200, 0.8461],\n",
      "        [0.9730, 0.4813, 0.8039],\n",
      "        [0.8248, 0.0035, 0.1573],\n",
      "        [0.6869, 0.3448, 0.9750]], requires_grad=True)\n",
      "\n",
      "\n",
      " ====================\n",
      "\n",
      "\n",
      "Model with with xavier weights are:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.0704, -0.3514, -0.3508],\n",
      "        [ 0.4697,  0.1831, -0.3544],\n",
      "        [-0.6440,  0.6279,  0.1374],\n",
      "        [ 0.3790, -0.1194, -0.6463],\n",
      "        [-0.5820, -0.5452,  0.5042],\n",
      "        [ 0.5086, -0.3400, -0.4402],\n",
      "        [-0.1331,  0.1536,  0.0938],\n",
      "        [-0.3489, -0.6314, -0.6142],\n",
      "        [-0.5964, -0.1057, -0.6694],\n",
      "        [-0.2208, -0.4703,  0.4866]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1606,  0.3820, -0.1451],\n",
      "        [ 0.1033,  0.0237, -0.3604],\n",
      "        [-0.6722, -0.6102,  0.4378],\n",
      "        [-0.0319, -0.4266,  0.5123],\n",
      "        [-0.1884, -0.3410,  0.0900],\n",
      "        [ 0.4293, -0.2774, -0.6427],\n",
      "        [-0.2514, -0.0837, -0.2616],\n",
      "        [-0.1142, -0.0427,  0.3790],\n",
      "        [ 0.0762,  0.4004,  0.6483],\n",
      "        [ 0.3815, -0.4611,  0.3489]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MatrixFactorizationWithoutBiasNoXavier(nn.Module):\n",
    "    def __init__(self, num_people, num_partners, weights=(0, 1), emb_size=100):\n",
    "        super(MatrixFactorizationWithoutBiasNoXavier, self).__init__()\n",
    "        self.person_emb = nn.Embedding(num_people, emb_size)\n",
    "        self.partner_emb = nn.Embedding(num_partners, emb_size)\n",
    "        self.person_emb.weight.data.uniform_(weights[0], weights[1])\n",
    "        self.partner_emb.weight.data.uniform_(weights[0], weights[1])\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        u = self.person_emb(u)\n",
    "        v = self.partner_emb(v)\n",
    "        # calculate dot product\n",
    "        # u*v is a element wise vector multiplication\n",
    "        return torch.sigmoid((u*v).sum(1))\n",
    "\n",
    "    \n",
    "class MatrixFactorizationWithoutBiasXavier(nn.Module):\n",
    "    def __init__(self, num_people, num_partners, emb_size=100):\n",
    "        super(MatrixFactorizationWithoutBiasXavier, self).__init__()\n",
    "        self.person_emb = nn.Embedding(num_people, emb_size)\n",
    "        self.partner_emb = nn.Embedding(num_partners, emb_size)\n",
    "        torch.nn.init.xavier_uniform_(self.person_emb.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.partner_emb.weight)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        u = self.person_emb(u)\n",
    "        v = self.partner_emb(v)\n",
    "        # calculate dot product\n",
    "        # u*v is a element wise vector multiplication\n",
    "        return torch.sigmoid((u*v).sum(1))\n",
    "\n",
    "    \n",
    "# Example small models demonstrating weights\n",
    "example_model_no_xavier = MatrixFactorizationWithoutBiasNoXavier(10, 10, emb_size=3)\n",
    "example_model_xavier = MatrixFactorizationWithoutBiasXavier(10, 10, emb_size=3)\n",
    "print(\"Model with without xavier weights are:\\n\")\n",
    "for p in example_model_no_xavier.parameters():\n",
    "    print(p)\n",
    "print('\\n\\n', '='*20)\n",
    "print(\"\\n\\nModel with with xavier weights are:\\n\")\n",
    "for p in example_model_xavier.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix factorization without bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with without xavier weights are:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.8983, 0.6825, 0.9373],\n",
      "        [0.8572, 0.8243, 0.0581],\n",
      "        [0.5317, 0.8954, 0.8672],\n",
      "        [0.5341, 0.1652, 0.4773],\n",
      "        [0.5631, 0.7309, 0.2486],\n",
      "        [0.6384, 0.0900, 0.8216],\n",
      "        [0.2206, 0.4834, 0.4217],\n",
      "        [0.7764, 0.6311, 0.2617],\n",
      "        [0.7911, 0.2562, 0.0534],\n",
      "        [0.2315, 0.7981, 0.1845]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.7202, 0.7536, 0.9493],\n",
      "        [0.4106, 0.1707, 0.9955],\n",
      "        [0.5313, 0.6219, 0.0362],\n",
      "        [0.2618, 0.4356, 0.3951],\n",
      "        [0.9732, 0.4318, 0.4953],\n",
      "        [0.6556, 0.9228, 0.4639],\n",
      "        [0.0241, 0.6854, 0.9229],\n",
      "        [0.0644, 0.3784, 0.9724],\n",
      "        [0.4314, 0.9780, 0.4291],\n",
      "        [0.6382, 0.1498, 0.2381]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "\n",
      "\n",
      " ====================\n",
      "\n",
      "\n",
      "Model with with xavier weights are:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.2183,  0.3249,  0.4007],\n",
      "        [-0.4951,  0.3654, -0.6530],\n",
      "        [-0.6610, -0.0415,  0.6506],\n",
      "        [ 0.3393,  0.2925,  0.4593],\n",
      "        [-0.1632, -0.4204,  0.4734],\n",
      "        [ 0.2080,  0.1229,  0.4635],\n",
      "        [ 0.3498,  0.3375, -0.0791],\n",
      "        [-0.6619,  0.6217,  0.4326],\n",
      "        [ 0.4363, -0.4923,  0.3279],\n",
      "        [ 0.0982,  0.4895, -0.2829]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0029],\n",
      "        [-0.0045],\n",
      "        [-0.0012],\n",
      "        [-0.0028],\n",
      "        [-0.0077],\n",
      "        [ 0.0073],\n",
      "        [ 0.0010],\n",
      "        [ 0.0088],\n",
      "        [ 0.0040],\n",
      "        [ 0.0093]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4093,  0.5071, -0.4496],\n",
      "        [ 0.4951,  0.2561, -0.4409],\n",
      "        [ 0.3358,  0.0446,  0.5637],\n",
      "        [ 0.0975,  0.4116,  0.5444],\n",
      "        [ 0.0055,  0.4311, -0.3887],\n",
      "        [ 0.5958, -0.3057,  0.3127],\n",
      "        [ 0.2036,  0.6075,  0.6007],\n",
      "        [ 0.3969, -0.0778,  0.3839],\n",
      "        [ 0.0598, -0.2970, -0.0674],\n",
      "        [ 0.4654, -0.4801,  0.5070]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0090],\n",
      "        [-0.0011],\n",
      "        [-0.0093],\n",
      "        [ 0.0050],\n",
      "        [-0.0047],\n",
      "        [ 0.0078],\n",
      "        [-0.0012],\n",
      "        [-0.0037],\n",
      "        [ 0.0038],\n",
      "        [ 0.0092]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MatrixFactorizationWithBiasNoXavier(nn.Module):\n",
    "    def __init__(self, num_people, num_partners, weights=(0, 1), bias=(-0.01, 0.01), emb_size=100):\n",
    "        super(MatrixFactorizationWithBiasNoXavier, self).__init__()\n",
    "        self.person_emb = nn.Embedding(num_people, emb_size)\n",
    "        self.person_bias = nn.Embedding(num_people, 1)\n",
    "        self.partner_emb = nn.Embedding(num_partners, emb_size)\n",
    "        self.parnter_bias = nn.Embedding(num_partners, 1)\n",
    "        self.person_emb.weight.data.uniform_(weights[0], weights[1])\n",
    "        self.partner_emb.weight.data.uniform_(weights[0], weights[1])\n",
    "        self.person_bias.weight.data.uniform_(bias[0], bias[1])\n",
    "        self.parnter_bias.weight.data.uniform_(bias[0], bias[1])\n",
    "            \n",
    "    def forward(self, u, v):\n",
    "        u = self.person_emb(u)\n",
    "        v = self.partner_emb(v)\n",
    "        bias_u = self.person_bias(u).squeeze()\n",
    "        bias_v = self.parnter_bias(v).squeeze()\n",
    "        # calculate dot product\n",
    "        # u*v is a element wise vector multiplication\n",
    "        return torch.sigmoid((u*v).sum(1) + bias_u + bias_v)\n",
    "    \n",
    "    \n",
    "class MatrixFactorizationWithBiasXavier(nn.Module):\n",
    "    def __init__(self, num_people, num_partners, bias=(-0.01, 0.01), emb_size=100):\n",
    "        super(MatrixFactorizationWithBiasXavier, self).__init__()\n",
    "        self.person_emb = nn.Embedding(num_people, emb_size)\n",
    "        self.person_bias = nn.Embedding(num_people, 1)\n",
    "        self.partner_emb = nn.Embedding(num_partners, emb_size)\n",
    "        self.parnter_bias = nn.Embedding(num_partners, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.person_emb.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.partner_emb.weight)\n",
    "        self.person_bias.weight.data.uniform_(bias[0], bias[1])\n",
    "        self.parnter_bias.weight.data.uniform_(bias[0], bias[1])\n",
    "            \n",
    "    def forward(self, u, v):\n",
    "        u = self.person_emb(u)\n",
    "        v = self.partner_emb(v)\n",
    "        bias_u = self.person_bias(u).squeeze()\n",
    "        bias_v = self.parnter_bias(v).squeeze()\n",
    "        # calculate dot product\n",
    "        # u*v is a element wise vector multiplication\n",
    "        return torch.sigmoid((u*v).sum(1) + bias_u + bias_v)\n",
    "    \n",
    "\n",
    "# Example small models demonstrating weights\n",
    "example_model_no_xavier = MatrixFactorizationWithBiasNoXavier(10, 10, bias=(0, 0), emb_size=3)\n",
    "example_model_xavier = MatrixFactorizationWithBiasXavier(10, 10, emb_size=3)\n",
    "print(\"Model with without xavier weights are:\\n\")\n",
    "for p in example_model_no_xavier.parameters():\n",
    "    print(p)\n",
    "print('\\n\\n', '='*20)\n",
    "print(\"\\n\\nModel with with xavier weights are:\\n\")\n",
    "for p in example_model_xavier.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, df_test, verbose=False):\n",
    "    model.eval()\n",
    "    # .to(dev) puts code on either gpu or cpu.\n",
    "    people = torch.LongTensor(df_test.id.values).to(dev)\n",
    "    partners = torch.LongTensor(df_test.pid.values).to(dev)\n",
    "    decision = torch.FloatTensor(df_test.decision.values).to(dev)\n",
    "    y_hat = model(people, partners)\n",
    "    loss = F.mse_loss(y_hat, decision)\n",
    "    if verbose:\n",
    "        print('test loss %.3f ' % loss.item())\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Default values assigned below are ones that I found online.\n",
    "# Cross validadtion will be done later but it's good to have some defaults.\n",
    "def train(model, df_train, epochs=100, learning_rate=0.01, weight_decay=1e-5, verbose=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # From numpy to PyTorch tensors.\n",
    "        # .to(dev) puts code on either gpu or cpu.\n",
    "        people = torch.LongTensor(df_train.id.values).to(dev)\n",
    "        partners = torch.LongTensor(df_train.pid.values).to(dev)\n",
    "        decision = torch.FloatTensor(df_train.decision.values).to(dev)\n",
    "        \n",
    "        # calls forward method of the model\n",
    "        y_hat = model(people, partners)\n",
    "        # Using mean squared errors loss function\n",
    "        loss = F.mse_loss(y_hat, decision)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and epoch % 100 == 0: \n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's finally train models and choose the best one\n",
    "\n",
    "## Let's first train men_like_women"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs_poss = list(range(10, 310, 10))\n",
    "weight_decay_poss = [0.0, 1e-6, 1e-5, 1e-4, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "learning_rate_poss = [1e-6, 1e-5, 1e-4, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "weights_poss = [(0, 1), (-1, 1), (0, 0.2), (-0.2, 0.2), 'xavier']\n",
    "bias_poss = [None, (0, 0), (-0.01, 0.01), (0, 1), (-1, 1)]\n",
    "emb_size_poss = [10, 50, 100, 150]\n",
    "\n",
    "def cross_vaild_model(model, train_df, test_df, verbose=False):\n",
    "    \"\"\"Function to choose the best hyperparameters for a model.\"\"\"\n",
    "    min_loss = float('inf')\n",
    "    best_settings = None\n",
    "\n",
    "    for (epochs, wd, lr) in product(epochs_poss, weight_decay_poss, learning_rate_poss):\n",
    "        train(model, train_df, epochs=epochs, learning_rate=lr, weight_decay=wd)\n",
    "        test_loss = test(model, test_df)\n",
    "        if (test_loss < min_loss) or (test_loss == min_loss and best_settings is not None and epochs < best_settings['epochs']):\n",
    "            min_loss = test_loss\n",
    "            best_settings = {'epochs': epochs, 'weight_decay': wd, 'learning_rate_poss': lr}\n",
    "    if verbose:\n",
    "        print('min loss %.3f' % min_loss)\n",
    "        print('best settings are', best_settings)\n",
    "    return min_loss, best_settings\n",
    "\n",
    "\n",
    "def cross_vaild(num_people, num_partners, train_df, test_df, verbose=False):\n",
    "    \"\"\"Function to choose the best model.\"\"\"\n",
    "    min_loss = float('inf')\n",
    "    best_settings = None\n",
    "    \n",
    "    for (weights, bias, emb_size) in tqdm(product(weights_poss, bias_poss, emb_size_poss)):\n",
    "        model = None\n",
    "        if weights == 'xavier':\n",
    "            if bias is None:\n",
    "                model = MatrixFactorizationWithoutBiasXavier(num_people, num_partners, emb_size=emb_size).to(dev)\n",
    "            else:\n",
    "                model = MatrixFactorizationWithBiasXavier(num_people, num_partners, bias=bias, emb_size=emb_size).to(dev)\n",
    "        else:\n",
    "            if bias is None:\n",
    "                model = MatrixFactorizationWithoutBiasNoXavier(num_people, num_partners, weights=weights, emb_size=emb_size).to(dev)\n",
    "            else:\n",
    "                model = MatrixFactorizationWithBiasNoXavier(num_people, num_partners, weights=weights, bias=bias, emb_size=emb_size).to(dev)\n",
    "        \n",
    "        model_min_loss, model_best_settings = cross_vaild_model(model, train_df, test_df, verbose)\n",
    "        if (model_min_loss < min_loss) or (model_min_loss == min_loss and best_settings and emb_size < best_settings['emb_size']):\n",
    "            min_loss = model_min_loss\n",
    "            curr_settings = {'model': model, 'weights': weights, 'bias': bias, 'emb_size': emb_size}\n",
    "            best_settings = {**curr_settings, **model_best_settings}\n",
    "            \n",
    "    return min_loss, best_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:07:49, 1017.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-7979379bce54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_vaild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmen_like_women_no_men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmen_like_women_no_women\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmen_like_women_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmen_like_women_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model = MatrixFactorizationWithoutBiasNoXavier(men_like_women_no_men, men_like_women_no_women).to(dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(cross_vaild_model(model, men_like_women_train_df, men_like_women_test_df))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-d3e3cdfa3ea1>\u001b[0m in \u001b[0;36mcross_vaild\u001b[0;34m(num_people, num_partners, train_df, test_df, verbose)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatrixFactorizationWithBiasNoXavier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_people\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_partners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmodel_min_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_best_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_vaild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_min_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_min_loss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmin_loss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbest_settings\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0memb_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emb_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mmin_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_min_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-d3e3cdfa3ea1>\u001b[0m in \u001b[0;36mcross_vaild_model\u001b[0;34m(model, train_df, test_df, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_poss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_poss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_poss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmin_loss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbest_settings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-eb524705ddbd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, df_train, epochs, learning_rate, weight_decay, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# min_loss, best_settings = cross_vaild(men_like_women_no_men, men_like_women_no_women, men_like_women_train_df, men_like_women_test_df)\n",
    "# print(min_loss)\n",
    "# print(best_settings)\n",
    "# model = MatrixFactorizationWithoutBiasNoXavier(men_like_women_no_men, men_like_women_no_women).to(dev)\n",
    "# print(cross_vaild_model(model, men_like_women_train_df, men_like_women_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# print(model(torch.LongTensor(men_like_women_train_df.id).to(dev), torch.LongTensor(men_like_women_train_df.pid).to(dev)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
