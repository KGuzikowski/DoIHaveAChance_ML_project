{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "from time import time\n",
    "from audtorch.metrics.functional import pearsonr\n",
    "\n",
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-07d33f3db2e9>:59: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  multiplier = 100/row.sum()\n",
      "<ipython-input-11-07d33f3db2e9>:61: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  base_df.at[x, col] *= multiplier\n"
     ]
    }
   ],
   "source": [
    "# Data preparation: \n",
    "# Some data is missing (but we know what it should be) or some data should be changed.\n",
    "base_df = pd.read_csv('./speed_dating_data.csv', encoding = 'ISO-8859-1')\n",
    "\n",
    "goal_c = {\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 0,\n",
    "    5: 1,\n",
    "    6: 1,\n",
    "}\n",
    "\n",
    "career_c = {\n",
    "    'lawyer': 1.,\n",
    "    'law': 1.,\n",
    "    'Economics': 7.,\n",
    "    'Economist': 7.,\n",
    "    'tech professional': 5.\n",
    "}\n",
    "\n",
    "for x, row in base_df.iterrows():\n",
    "    race = base_df.at[x, 'race']\n",
    "    race_o = base_df.at[x, 'race_o']\n",
    "    date = base_df.at[x, 'date']\n",
    "    go_out = base_df.at[x, 'go_out']\n",
    "    goal = base_df.at[x, 'goal']\n",
    "    career = base_df.at[x, 'career']\n",
    "    \n",
    "    if career in career_c:\n",
    "        base_df.at[x, 'career_c'] = career_c[career]\n",
    "    if str(career) == 'nan':\n",
    "        # if carrer is nan then change its code for 18 which means 'Other'\n",
    "        base_df.at[x, 'career_c'] = 18.    \n",
    "    if goal in goal_c:\n",
    "        base_df.at[x, 'goal'] = goal_c[goal]\n",
    "    if str(race) == 'nan':\n",
    "        # if race is nan then change its code for 6 which means 'Other'\n",
    "        base_df.at[x, 'race'] = 6.\n",
    "    if str(race_o) == 'nan':\n",
    "        # if race is nan then change its code for 6 which means 'Other'\n",
    "        base_df.at[x, 'race_o'] = 6.\n",
    "    if str(date) == 'nan':\n",
    "        # if date is nan then change its code for 8 which means 'Other'\n",
    "        base_df.at[x, 'date'] = 8.\n",
    "    if str(go_out) == 'nan':\n",
    "        # if go_out is nan then change its code for 8 which means 'Other'\n",
    "        base_df.at[x, 'go_out'] = 8.\n",
    "\n",
    "base_df.drop(['career'], axis=1, inplace=True)\n",
    "        \n",
    "# data is inconsistent:\n",
    "# Some values (itâ€™s based on waves) are 1-10 and others are 1-100 so\n",
    "# they should be normalized so that all are 1-100.\n",
    "\n",
    "def normalize(lst):\n",
    "    selected_df = base_df[lst]\n",
    "    for x, row in selected_df.iterrows():\n",
    "        multiplier = 100/row.sum()\n",
    "        for col in lst:\n",
    "            base_df.at[x, col] *= multiplier\n",
    "\n",
    "lst1 = ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']\n",
    "lst2 = ['attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o']\n",
    "lst3 = ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']\n",
    "\n",
    "normalize(lst1)\n",
    "normalize(lst2)\n",
    "normalize(lst3)\n",
    "\n",
    "# Column met_o had values 1/2. It needs to be changed to 0/1.\n",
    "base_df['met_o'] = base_df['met_o'].apply(lambda x: x - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed some rows (by hand) because they didn't have `pid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>gender</th>\n",
       "      <th>pid</th>\n",
       "      <th>age_o</th>\n",
       "      <th>pf_o_att</th>\n",
       "      <th>pf_o_sin</th>\n",
       "      <th>pf_o_int</th>\n",
       "      <th>pf_o_fun</th>\n",
       "      <th>pf_o_amb</th>\n",
       "      <th>pf_o_sha</th>\n",
       "      <th>...</th>\n",
       "      <th>date_5.0</th>\n",
       "      <th>date_6.0</th>\n",
       "      <th>date_7.0</th>\n",
       "      <th>go_out_1.0</th>\n",
       "      <th>go_out_2.0</th>\n",
       "      <th>go_out_3.0</th>\n",
       "      <th>go_out_4.0</th>\n",
       "      <th>go_out_5.0</th>\n",
       "      <th>go_out_6.0</th>\n",
       "      <th>go_out_7.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "      <td>526.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "      <td>527.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "      <td>528.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "      <td>529.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "      <td>530.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8378 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      iid  gender    pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       "0       1       0   11.0   27.0      35.0      20.0      20.0      20.0   \n",
       "1       1       0   12.0   22.0      60.0       0.0       0.0      40.0   \n",
       "2       1       0   13.0   22.0      19.0      18.0      19.0      18.0   \n",
       "3       1       0   14.0   23.0      30.0       5.0      15.0      40.0   \n",
       "4       1       0   15.0   24.0      30.0      10.0      20.0      10.0   \n",
       "...   ...     ...    ...    ...       ...       ...       ...       ...   \n",
       "8373  552       1  526.0   26.0      10.0      10.0      30.0      20.0   \n",
       "8374  552       1  527.0   24.0      50.0      20.0      10.0       5.0   \n",
       "8375  552       1  528.0   29.0      40.0      10.0      30.0      10.0   \n",
       "8376  552       1  529.0   22.0      10.0      25.0      25.0      10.0   \n",
       "8377  552       1  530.0   22.0      20.0      20.0      10.0      15.0   \n",
       "\n",
       "      pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
       "0          0.0       5.0  ...         0         0         1           1   \n",
       "1          0.0       0.0  ...         0         0         1           1   \n",
       "2         14.0      12.0  ...         0         0         1           1   \n",
       "3          5.0       5.0  ...         0         0         1           1   \n",
       "4         10.0      20.0  ...         0         0         1           1   \n",
       "...        ...       ...  ...       ...       ...       ...         ...   \n",
       "8373      10.0      15.0  ...         0         0         0           1   \n",
       "8374      10.0       5.0  ...         0         0         0           1   \n",
       "8375      10.0       NaN  ...         0         0         0           1   \n",
       "8376      10.0      20.0  ...         0         0         0           1   \n",
       "8377       5.0      30.0  ...         0         0         0           1   \n",
       "\n",
       "      go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
       "0              0           0           0           0           0           0  \n",
       "1              0           0           0           0           0           0  \n",
       "2              0           0           0           0           0           0  \n",
       "3              0           0           0           0           0           0  \n",
       "4              0           0           0           0           0           0  \n",
       "...          ...         ...         ...         ...         ...         ...  \n",
       "8373           0           0           0           0           0           0  \n",
       "8374           0           0           0           0           0           0  \n",
       "8375           0           0           0           0           0           0  \n",
       "8376           0           0           0           0           0           0  \n",
       "8377           0           0           0           0           0           0  \n",
       "\n",
       "[8378 rows x 110 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# career\n",
    "base_df = pd.concat([base_df, pd.get_dummies(base_df['career_c'], prefix='career', drop_first=True)], axis=1)\n",
    "base_df.drop(['career_c'], axis=1, inplace=True)\n",
    "\n",
    "# race and race_o\n",
    "race_df = pd.get_dummies(base_df['race'], prefix='race')\n",
    "race_df.drop(['race_6.0'], axis=1, inplace=True)\n",
    "base_df = pd.concat([base_df, race_df], axis=1)\n",
    "base_df.drop(['race'], axis=1, inplace=True)\n",
    "\n",
    "race_o_df = pd.get_dummies(base_df['race_o'], prefix='race_o')\n",
    "race_o_df.drop(['race_o_6.0'], axis=1, inplace=True)\n",
    "base_df = pd.concat([base_df, race_o_df], axis=1)\n",
    "base_df.drop(['race_o'], axis=1, inplace=True)\n",
    "\n",
    "# date\n",
    "date_df = pd.get_dummies(base_df['date'], prefix='date')\n",
    "date_df.drop(['date_8.0'], axis=1, inplace=True)\n",
    "base_df = pd.concat([base_df, date_df], axis=1)\n",
    "base_df.drop(['date'], axis=1, inplace=True)\n",
    "base_df\n",
    "\n",
    "# go_out\n",
    "go_out_df = pd.get_dummies(base_df['go_out'], prefix='go_out')\n",
    "go_out_df.drop(['go_out_8.0'], axis=1, inplace=True)\n",
    "base_df = pd.concat([base_df, go_out_df], axis=1)\n",
    "base_df.drop(['go_out'], axis=1, inplace=True)\n",
    "\n",
    "base_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below you can see code that prepares data for the basic matrix factorization.\n",
    "Here in the base matrix we only have information about match. So we need a set of vectors, where each vector describes each date (holds both ids and information about match).\n",
    "\n",
    "Task will requite two base matrices.\n",
    "1. Matrix where men are \"users\" and women are \"products\". It will then be used to recommend women to men because matrix will say what's the predicted rating of a woman in eyes of man. Basically it will answer the question: **\"How likely is that a man will like a woman?\"**. Let's call this matrix/data frame **\"men_like_women\"**.\n",
    "1. Matrix where women are \"users\" and men are \"products\". It will then be used to recommend men to women because matrix will say what's the predicted rating of a man in eyes of woman. Basically it will answer the question: **\"How likely is that a woman will like a man?\"**. Let's call this matrix/data frame **\"women_like_men\"**.\n",
    "\n",
    "Why such analogies? It may help to understand how do this human relations task translates into recommender systems world.\n",
    "\n",
    "These matrices will be used to train models (with matrix factorization) which will then be saved into csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "men_like_women_df:\n",
      "         id    pid  decision\n",
      "0      11.0    1.0       0.0\n",
      "1      11.0    2.0       0.0\n",
      "2      11.0    3.0       0.0\n",
      "3      11.0    4.0       0.0\n",
      "4      11.0    5.0       0.0\n",
      "...     ...    ...       ...\n",
      "4189  552.0  526.0       0.0\n",
      "4190  552.0  527.0       0.0\n",
      "4191  552.0  528.0       0.0\n",
      "4192  552.0  529.0       0.0\n",
      "4193  552.0  530.0       0.0\n",
      "\n",
      "[4194 rows x 3 columns]\n",
      "\n",
      "women_like_men_df:\n",
      "         id    pid  decision\n",
      "0       1.0   11.0       1.0\n",
      "1       1.0   12.0       1.0\n",
      "2       1.0   13.0       1.0\n",
      "3       1.0   14.0       1.0\n",
      "4       1.0   15.0       1.0\n",
      "...     ...    ...       ...\n",
      "4179  530.0  548.0       0.0\n",
      "4180  530.0  549.0       1.0\n",
      "4181  530.0  550.0       0.0\n",
      "4182  530.0  551.0       0.0\n",
      "4183  530.0  552.0       1.0\n",
      "\n",
      "[4184 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split into vectors, let's have two matrices as describes above.\n",
    "men_like_women_data = []\n",
    "women_like_men_data = []\n",
    "\n",
    "for _, row in base_df.iterrows():\n",
    "    vector = {\n",
    "        'id': row['iid'],\n",
    "        'pid': row['pid'],\n",
    "        'decision': row['dec'],\n",
    "    }\n",
    "    if row['gender'] == 0:\n",
    "        # it's a woman\n",
    "        women_like_men_data.append(vector)\n",
    "    else:\n",
    "        men_like_women_data.append(vector)\n",
    "\n",
    "men_like_women_df = pd.DataFrame(men_like_women_data)\n",
    "women_like_men_df = pd.DataFrame(women_like_men_data)\n",
    "\n",
    "print(\"men_like_women_df:\")\n",
    "print(men_like_women_df)\n",
    "print(\"\\nwomen_like_men_df:\")\n",
    "print(women_like_men_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare train and test data sets\n",
    "\n",
    "### Make ids contiguous\n",
    "We need to change the data. As you can see ids are not contiguous and they must be. So let's change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_col(column):\n",
    "    \"\"\"Encodes a pandas column with continous ids.\"\"\"\n",
    "    unique = column.unique()\n",
    "    old_to_new = {o: i for i, o in enumerate(unique)}\n",
    "    return np.array([old_to_new.get(x, -1) for x in column])\n",
    "\n",
    "\n",
    "def encode_data(df):\n",
    "    \"\"\"Encodes data with continous person and partner ids.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col_name in [\"id\", \"pid\"]:\n",
    "        col = proc_col(df[col_name])\n",
    "        df[col_name] = col\n",
    "        df = df[df[col_name] >= 0]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now use these functions to make ids contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "         id    pid  decision\n",
      "0      11.0    1.0       0.0\n",
      "1      11.0    2.0       0.0\n",
      "2      11.0    3.0       0.0\n",
      "3      11.0    4.0       0.0\n",
      "4      11.0    5.0       0.0\n",
      "...     ...    ...       ...\n",
      "4189  552.0  526.0       0.0\n",
      "4190  552.0  527.0       0.0\n",
      "4191  552.0  528.0       0.0\n",
      "4192  552.0  529.0       0.0\n",
      "4193  552.0  530.0       0.0\n",
      "\n",
      "[4194 rows x 3 columns]\n",
      "\n",
      "After:\n",
      "       id  pid  decision\n",
      "0       0    0       0.0\n",
      "1       0    1       0.0\n",
      "2       0    2       0.0\n",
      "3       0    3       0.0\n",
      "4       0    4       0.0\n",
      "...   ...  ...       ...\n",
      "4189  276  270       0.0\n",
      "4190  276  271       0.0\n",
      "4191  276  272       0.0\n",
      "4192  276  273       0.0\n",
      "4193  276  274       0.0\n",
      "\n",
      "[4184 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print('Before:')\n",
    "print(men_like_women_df)\n",
    "men_like_women_df = encode_data(men_like_women_df)\n",
    "print('\\nAfter:')\n",
    "print(men_like_women_df)\n",
    "\n",
    "women_like_men_df = encode_data(women_like_men_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split into train and test data sets\n",
    "Standard sklearn function `train_test_split` doesn't do a job here because both test and train data sets should include all people. So after using `train_test_split` we need to transition some people between sets to ensure that both sets are correct. The same goes for partners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "def train_test_split(df, test_size=0.2):\n",
    "    # Use standard train_test_split.\n",
    "    df_train, df_test = tts(df, test_size=test_size)\n",
    "    no_train_unique = len(df_train.id.unique())\n",
    "    no_test_unique = len(df_test.id.unique())\n",
    "    \n",
    "    # See what people are missing in the test set.\n",
    "    diff = np.setdiff1d(df_train.id.unique(), df_test.id.unique())\n",
    "    for id in diff:\n",
    "        # For every missing person we need to exchange them for a\n",
    "        # different one so that sets still have the same number of elements.\n",
    "        person_to_send_to_test = df_train.loc[df_train.id == id].iloc[0]\n",
    "        person_to_send_to_train = None\n",
    "        # Choose some person from the test set to send to the train set.\n",
    "        ids = df_test.id.unique()\n",
    "        np.random.shuffle(ids)\n",
    "        for test_id in ids:\n",
    "            person = df_test.loc[df_test.id == test_id]\n",
    "            if len(person.index) > 1:\n",
    "                person_to_send_to_train = person.iloc[0]\n",
    "                break\n",
    "        if person_to_send_to_train is not None:\n",
    "            # Remove people that transit from old sets.\n",
    "            # .name holds the id of that row in the oryginal df.\n",
    "            df_train = df_train.drop(person_to_send_to_test.name)\n",
    "            df_test = df_test.drop(person_to_send_to_train.name)\n",
    "            # Add new people to sets.\n",
    "            df_train = pd.concat([df_train, person_to_send_to_train.to_frame().T], ignore_index=True)\n",
    "            df_test = pd.concat([df_test, person_to_send_to_test.to_frame().T], ignore_index=True)\n",
    "        else:\n",
    "            raise Exception(\"Couldn't find any person from people to send from the test to the train.\")\n",
    "        \n",
    "    # See what partners are missing in the test set.\n",
    "    diff = np.setdiff1d(df_train.pid.unique(), df_test.pid.unique())\n",
    "    for pid in diff:\n",
    "        # For every missing partner we need to exchange them for a\n",
    "        # different one so that sets still have the same number of elements.\n",
    "        partner_to_send_to_test = df_train.loc[df_train.pid == pid].iloc[0]\n",
    "        partner_to_send_to_train = None\n",
    "        \n",
    "        # Choose some partner from the test set to send to the train set.\n",
    "        # Need to make sure that both sets will still have all the people.\n",
    "        pids = df_test.pid.unique()\n",
    "        np.random.shuffle(pids)\n",
    "        for test_pid in pids:\n",
    "            partner = df_test.loc[df_test.pid == test_pid]\n",
    "            if len(partner.index) > 1:\n",
    "                # Make sure we don't remove a person completely.\n",
    "                id = partner.iloc[0].id\n",
    "                person_qty = len(df_test.loc[df_test.id == id].index)\n",
    "                if person_qty > 1:\n",
    "                    partner_to_send_to_train = partner.iloc[0]\n",
    "                    break\n",
    "        if partner_to_send_to_train is not None:\n",
    "            # Remove partners that transit from old sets.\n",
    "            df_train = df_train.drop(partner_to_send_to_test.name)\n",
    "            df_test = df_test.drop(partner_to_send_to_train.name)\n",
    "            # Add new people to sets.\n",
    "            df_train = pd.concat([df_train, partner_to_send_to_train.to_frame().T], ignore_index=True)\n",
    "            df_test = pd.concat([df_test, partner_to_send_to_test.to_frame().T], ignore_index=True)\n",
    "        else:\n",
    "            raise Exception(\"Couldn't find any partner from partners to send from the test to the train.\")\n",
    "                    \n",
    "    df_train = df_train.sort_values(by='id')\n",
    "    df_test = df_test.sort_values(by='id')\n",
    "    return df_train.reset_index(drop=True), df_test.sort_values(by='id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now just get correct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_like_women_train_df, men_like_women_test_df = train_test_split(men_like_women_df, test_size=0.2)\n",
    "men_like_women_no_men = len(men_like_women_train_df.id.unique())\n",
    "men_like_women_no_women = len(men_like_women_train_df.pid.unique())\n",
    "\n",
    "women_like_men_train_df, women_like_men_test_df = train_test_split(women_like_men_df, test_size=0.2)\n",
    "women_like_men_no_women = len(women_like_men_train_df.id.unique())\n",
    "women_like_men_no_men = len(women_like_men_train_df.pid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create matrix factorization models\n",
    "\n",
    "We will create and train several base MF (matrix factorization) models and for each of them let's do a cross validation to learn the best hyperparameters and parameters. Then we will compare the results and choose the best model.\n",
    "\n",
    "Some general explanations for models:\n",
    "* Models are train on only one batch because our data set is rather small.\n",
    "\n",
    "Good reading resource: https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dev = torch.device('cuda')\n",
    "dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights are:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.0762, 0.3123, 0.5546],\n",
      "        [0.1057, 0.1367, 0.5152],\n",
      "        [0.1282, 0.2517, 0.2468],\n",
      "        [0.8584, 0.1111, 0.6178],\n",
      "        [0.8935, 0.5006, 0.5250],\n",
      "        [0.0151, 0.9112, 0.3072],\n",
      "        [0.7003, 0.3894, 0.4643],\n",
      "        [0.4291, 0.0807, 0.4183],\n",
      "        [0.3528, 0.5130, 0.0663],\n",
      "        [0.4358, 0.0360, 0.5397]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0843, 0.8679, 0.9682],\n",
      "        [0.2266, 0.3234, 0.7096],\n",
      "        [0.3889, 0.6726, 0.5233],\n",
      "        [0.5458, 0.2156, 0.0217],\n",
      "        [0.7662, 0.8410, 0.6444],\n",
      "        [0.2075, 0.1152, 0.2188],\n",
      "        [0.9869, 0.1833, 0.8718],\n",
      "        [0.8725, 0.1490, 0.9325],\n",
      "        [0.2922, 0.3397, 0.3623],\n",
      "        [0.1246, 0.9288, 0.3558]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "\n",
      "\n",
      " ====================\n"
     ]
    }
   ],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_people, num_partners, weights=(0, 1), bias=(-0.01, 0.01), emb_size=100):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.person_emb = nn.Embedding(num_people, emb_size)\n",
    "        self.person_bias = nn.Embedding(num_people, 1)\n",
    "        self.partner_emb = nn.Embedding(num_partners, emb_size)\n",
    "        self.parnter_bias = nn.Embedding(num_partners, 1)\n",
    "        self.person_emb.weight.data.uniform_(weights[0], weights[1])\n",
    "        self.partner_emb.weight.data.uniform_(weights[0], weights[1])\n",
    "        self.person_bias.weight.data.uniform_(bias[0], bias[1])\n",
    "        self.parnter_bias.weight.data.uniform_(bias[0], bias[1])\n",
    "            \n",
    "    def forward(self, u, v):\n",
    "        bias_u = self.person_bias(u).squeeze()\n",
    "        bias_v = self.parnter_bias(v).squeeze()\n",
    "        u = self.person_emb(u)\n",
    "        v = self.partner_emb(v)\n",
    "        # calculate dot product\n",
    "        # u*v is a element wise vector multiplication\n",
    "        return torch.sigmoid((u*v).sum(1) + bias_u + bias_v)\n",
    "    \n",
    "\n",
    "# Example small models demonstrating weights\n",
    "example_model = MatrixFactorization(10, 10, bias=(0, 0), emb_size=3)\n",
    "print(\"Model weights are:\\n\")\n",
    "for p in example_model.parameters():\n",
    "    print(p)\n",
    "print('\\n\\n', '='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MF_test(model, df_test, verbose=False):\n",
    "    model.eval()\n",
    "    # .to(dev) puts code on either gpu or cpu.\n",
    "    people = torch.LongTensor(df_test.id.values).to(dev)\n",
    "    partners = torch.LongTensor(df_test.pid.values).to(dev)\n",
    "    decision = torch.FloatTensor(df_test.decision.values).to(dev)\n",
    "    y_hat = model(people, partners)\n",
    "    loss = F.mse_loss(y_hat, decision)\n",
    "    if verbose:\n",
    "        print('test loss %.3f ' % loss.item())\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Default values assigned below are ones that I found online.\n",
    "# Cross validadtion will be done later but it's good to have some defaults.\n",
    "def MF_train(model, df_train, epochs=100, learning_rate=0.01, weight_decay=1e-5, verbose=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # From numpy to PyTorch tensors.\n",
    "        # .to(dev) puts code on either gpu or cpu.\n",
    "        people = torch.LongTensor(df_train.id.values).to(dev)\n",
    "        partners = torch.LongTensor(df_train.pid.values).to(dev)\n",
    "        decision = torch.FloatTensor(df_train.decision.values).to(dev)\n",
    "        \n",
    "        # calls forward method of the model\n",
    "        y_hat = model(people, partners)\n",
    "        # Using mean squared errors loss function\n",
    "        loss = F.mse_loss(y_hat, decision)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and epoch % 100 == 0: \n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's finally train models and choose the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_model(train_df, test_df, num_people, num_partners, bias, weights, emb_size):\n",
    "    \"\"\"Function to choose the best hyperparameters for a model.\"\"\"\n",
    "    min_loss = np.inf\n",
    "    best_settings = None\n",
    "\n",
    "    for (epochs, wd, lr) in product(epochs_poss, weight_decay_poss, learning_rate_poss):\n",
    "        model = MatrixFactorization(num_people, num_partners, weights=weights, bias=bias, emb_size=emb_size).to(dev)\n",
    "        MF_train(model, train_df, epochs=epochs, learning_rate=lr, weight_decay=wd)\n",
    "        test_loss = MF_test(model, test_df)\n",
    "        if (test_loss < min_loss) or (test_loss == min_loss and best_settings is not None and epochs < best_settings['epochs']):\n",
    "            min_loss = test_loss\n",
    "            best_settings = {'epochs': epochs, 'weight_decay': wd, 'learning_rate': lr}\n",
    "    return min_loss, best_settings\n",
    "\n",
    "\n",
    "def cross_valid(\n",
    "    num_people, num_partners, train_df, test_df,\n",
    "    epochs_poss,\n",
    "    weight_decay_poss,\n",
    "    learning_rate_poss,\n",
    "    weights_poss,\n",
    "    bias_poss,\n",
    "    emb_size_poss,\n",
    "    verbose=False, file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to choose the best model.\n",
    "    If arg file is provided (should be a path), statistics will be written to the given file.\n",
    "    \"\"\"\n",
    "    min_loss = np.inf\n",
    "    best_settings = None\n",
    "    models = []\n",
    "    start_time = time()\n",
    "    \n",
    "    for (weights, bias, emb_size) in tqdm(product(weights_poss, bias_poss, emb_size_poss)):            \n",
    "        model_min_loss, model_best_settings = cross_valid_model(train_df, test_df, num_people, num_partners, bias, weights, emb_size)\n",
    "        \n",
    "        curr_settings = {'min_loss': model_min_loss, 'weights': weights, 'bias': bias, 'emb_size': emb_size}\n",
    "        model_best_settings = {**curr_settings, **model_best_settings}\n",
    "        models.append(model_best_settings)\n",
    "            \n",
    "        if verbose:\n",
    "            print(f'best_settings: {model_best_settings}')\n",
    "            print(f'{\"=\"*50}')\n",
    "        \n",
    "        if (model_min_loss < min_loss) or (model_min_loss == min_loss and best_settings and emb_size < best_settings['emb_size']):\n",
    "            min_loss = model_min_loss\n",
    "            best_settings = model_best_settings\n",
    "    \n",
    "    end_time = time()\n",
    "        \n",
    "    if verbose:\n",
    "        print(f'\\n{\"?\"*50}')\n",
    "        print('THE WINNER IS:')\n",
    "        print(f'best min_loss: {min_loss}')\n",
    "        print(f'best best_settings: {best_settings}')\n",
    "        print(f'Cross validation took {end_time - start_time}')\n",
    "    \n",
    "    if file:\n",
    "        models.sort(key=lambda x: x['min_loss'])\n",
    "        with open(file, 'wb') as stats_file:\n",
    "            pickle.dump(models, stats_file)\n",
    "        \n",
    "    return min_loss, best_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first train women_like_man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_CROSS_VALID_RES_FILE = './women_like_men_MF_cross_vaild_results.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epochs_poss = list(range(1400, 1700, 100))\n",
    "epochs_poss = [1400]\n",
    "weight_decay_poss = [1e-7, 1e-6]\n",
    "learning_rate_poss = [0.001]\n",
    "weights_poss = [(0, 1)]\n",
    "bias_poss = [(-1, 1)]\n",
    "emb_size_poss = list(range(1900, 2400, 100))\n",
    "\n",
    "min_loss, best_settings = cross_valid(\n",
    "    women_like_men_no_women, women_like_men_no_men,\n",
    "    women_like_men_train_df, women_like_men_test_df,\n",
    "    epochs_poss=epochs_poss,\n",
    "    weight_decay_poss=weight_decay_poss,\n",
    "    learning_rate_poss=learning_rate_poss,\n",
    "    weights_poss=weights_poss,\n",
    "    bias_poss=bias_poss,\n",
    "    emb_size_poss=emb_size_poss,\n",
    "    file=MF_CROSS_VALID_RES_FILE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see results of the above cross valid. Models are sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(MF_CROSS_VALID_RES_FILE, 'rb') as file:\n",
    "    mf_cross_valid_results = pickle.load(file)\n",
    "            \n",
    "# sorting\n",
    "mf_cross_valid_results.sort(key=lambda x: x['min_loss'])\n",
    "mf_cv_res_pd = pd.DataFrame(mf_cross_valid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_loss</th>\n",
       "      <th>weights</th>\n",
       "      <th>bias</th>\n",
       "      <th>emb_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.179770</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.180538</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>1000</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.180655</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1200</td>\n",
       "      <td>1600</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.180667</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1100</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180749</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1000</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.180875</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1300</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.181202</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>1100</td>\n",
       "      <td>1300</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.181699</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>1200</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.181801</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>1100</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.182414</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>1300</td>\n",
       "      <td>1300</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_loss  weights     bias  emb_size  epochs  weight_decay  learning_rate\n",
       "0  0.179770  (-1, 1)   (0, 0)      1400    1400       0.00010          0.001\n",
       "1  0.180538   (0, 1)  (-1, 1)      1000    1200       0.00001          0.001\n",
       "2  0.180655  (-1, 1)   (0, 0)      1200    1600       0.00010          0.001\n",
       "3  0.180667  (-1, 1)   (0, 0)      1100    1500       0.00010          0.001\n",
       "4  0.180749  (-1, 1)   (0, 0)      1000    1400       0.00010          0.001\n",
       "5  0.180875  (-1, 1)   (0, 0)      1300    1500       0.00010          0.001\n",
       "6  0.181202   (0, 1)  (-1, 1)      1100    1300       0.00001          0.001\n",
       "7  0.181699   (0, 1)  (-1, 1)      1200    1400       0.00010          0.001\n",
       "8  0.181801  (-1, 1)  (-1, 1)      1100    1500       0.00010          0.001\n",
       "9  0.182414   (0, 1)  (-1, 1)      1300    1300       0.00001          0.001"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_cv_res_pd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the results above we found better set of parameters and hyperparameters (by hand because we ran out of time). So the winner is:\n",
    "\n",
    "* weights: (0, 1)\n",
    "* bias: (-1, 1)\n",
    "* emb_size: 2000\n",
    "* epochs: 1400\n",
    "* learning_rate: 0.001\n",
    "* weight_decay: 1e-6\n",
    "\n",
    "whose loss is 0.1758.\n",
    "\n",
    "To clarify, we know that this loss is the final correct loss, because it's overfitted, because test set = validation set. We did't have the time to create a custom methods for kfold. But don't worry, final usage of that will be cross validated with kfold which should give us reliable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create logistic regression\n",
    "Now, to create regression models, both used before and after, we will modify data set in such ways:\n",
    "\n",
    "1. Modified income column and $..._5_1$ columns - here the missing values are filled with the mean of the columns.\n",
    "2. With modified income column - here the missing values are filled with the mean/median of the column and removed $..._5_1$ columns.\n",
    "3. With modified $..._5_1$ columns - here the missing values are filled with the mean/median of the columns and removed income column.\n",
    "4. Removed income column and $..._5_1$ columns.\n",
    "\n",
    "#### Later we will aply the logic to combine this with MF.\n",
    "\n",
    "It's going to be used:\n",
    "1. Before matrix factorization - output of this model will be used as a input to MF.\n",
    "2. After matrix factorization - output of MF will be used as input in this model, as one of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(df, filling_data):\n",
    "    \"\"\"Function fills missing values with mean of the column\"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        for column in df.columns:\n",
    "            if column not in ['met_o', 'goal'] and np.isnan(row[column]):\n",
    "                df.at[index, column] = filling_data[column]\n",
    "            elif column == 'goal' and np.isnan(row['goal']):\n",
    "                df.at[index, column] = 1\n",
    "            elif column == 'met_o' and (np.isnan(row['met_o']) or row['met_o'] != 0):\n",
    "                df.at[index, column] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_with_mean(df):\n",
    "    col_means = df.mean()\n",
    "    return fill_data(df, col_means)\n",
    "\n",
    "\n",
    "def fill_with_median(df):\n",
    "    col_medians = df.median()\n",
    "    return fill_data(df, col_medians)\n",
    "\n",
    "\n",
    "women_like_men_LG_df = base_df.copy()\n",
    "women_like_men_LG_df = women_like_men_LG_df[women_like_men_LG_df['gender'] == 1]\n",
    "women_like_men_LG_df.drop(columns=['gender', 'iid', 'pid', 'dec'], inplace=True)\n",
    "women_like_men_LG_df = women_like_men_LG_df.reset_index(drop=True)\n",
    "\n",
    "poss_LG_data_sets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1\n",
    "Modified income column and  $..._5_1$  columns - here the missing values are filled with the mean of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG_no1_mean_df = base_df.copy()\n",
    "LG_no1_mean_df = fill_with_mean(LG_no1_mean_df)\n",
    "poss_LG_data_sets.append(LG_no1_mean_df)\n",
    "\n",
    "LG_no1_median_df = base_df.copy()\n",
    "LG_no1_median_df = fill_with_median(LG_no1_median_df)\n",
    "poss_LG_data_sets.append(LG_no1_median_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2\n",
    "With modified income column - here the missing values are filled with the mean/median of the column and removed  $..._5_1$  columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1']\n",
    "\n",
    "LG_no2_mean_df = base_df.copy()\n",
    "LG_no2_mean_df.drop(columns=cols_to_drop, inplace=True)\n",
    "LG_no2_mean_df = fill_with_mean(LG_no2_mean_df)\n",
    "poss_LG_data_sets.append(LG_no2_mean_df)\n",
    "\n",
    "LG_no2_median_df = base_df.copy()\n",
    "LG_no2_median_df.drop(columns=cols_to_drop, inplace=True)\n",
    "LG_no2_median_df = fill_with_median(LG_no2_median_df)\n",
    "poss_LG_data_sets.append(LG_no2_median_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3\n",
    "With modified  $..._5_1$  columns - here the missing values are filled with the mean/median of the columns and removed income column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['income']\n",
    "\n",
    "LG_no3_mean_df = base_df.copy()\n",
    "LG_no3_mean_df.drop(columns=cols_to_drop, inplace=True)\n",
    "LG_no3_mean_df = fill_with_mean(LG_no3_mean_df)\n",
    "poss_LG_data_sets.append(LG_no3_mean_df)\n",
    "\n",
    "LG_no3_median_df = base_df.copy()\n",
    "LG_no3_median_df.drop(columns=cols_to_drop, inplace=True)\n",
    "LG_no3_median_df = fill_with_median(LG_no3_median_df)\n",
    "poss_LG_data_sets.append(LG_no3_median_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 4\n",
    "Removed income column and $..._5_1$ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['income', 'attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1']\n",
    "\n",
    "LG_no4_df = base_df.copy()\n",
    "LG_no4_df.drop(columns=cols_to_drop, inplace=True)\n",
    "poss_LG_data_sets.append(LG_no4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        #(out)input_dim is size of our (out)input data\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred_y = torch.sigmoid(self.linear(x))\n",
    "        return torch.squeeze(pred_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_train(model, x, y, epochs=100, learning_rate=0.01, weight_decay=1e-5, verbose=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # From numpy to PyTorch tensors.\n",
    "        # .to(dev) puts code on either gpu or cpu.\n",
    "        X = torch.FloatTensor(x).to(dev)\n",
    "        Y = torch.FloatTensor(y).to(dev)\n",
    "        \n",
    "        # calls forward method of the model\n",
    "        y_hat = model(X)\n",
    "        # Using mean squared errors loss function\n",
    "        loss = criterion(y_hat, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and epoch % 10 == 0: \n",
    "            print(loss.item())\n",
    "            \n",
    "def LR_test(model, x, y, verbose=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    # .to(dev) puts code on either gpu or cpu.\n",
    "    X = torch.FloatTensor(x).to(dev)\n",
    "    Y = torch.FloatTensor(y).to(dev)\n",
    "    y_hat = model(X)\n",
    "    # Using mean squared errors loss function\n",
    "    loss = criterion(y_hat, Y)\n",
    "    if verbose:\n",
    "        print('test loss %.3f ' % loss.item())\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the best model using Cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing data before running cross validation to find optimal parameters for training MF for new date decisionepochs_poss = list(range(100, 510, 10))\n",
    "weight_decay_poss = [0.0, 1e-6, 1e-5, 1e-4, 0.0001, 0.001, 0.01, 0.1]\n",
    "learning_rate_poss = [1e-6, 1e-5, 1e-4, 0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "def cross_valid_regression(model, train_x, train_y, test_x, test_y, verbose=False):\n",
    "    \"\"\"Function to choose the best hyperparameters for a model.\"\"\"\n",
    "    min_loss = np.inf\n",
    "    best_settings = None\n",
    "\n",
    "    for (epochs, wd, lr) in tqdm(product(epochs_poss, weight_decay_poss, learning_rate_poss)):\n",
    "        train(model, train_x, train_y, epochs=epochs, learning_rate=lr, weight_decay=wd)\n",
    "        test_loss = test(model, test_x, test_y)\n",
    "        if (test_loss < min_loss) or (test_loss == min_loss and best_settings is not None and epochs < best_settings['epochs']):\n",
    "            min_loss = test_loss\n",
    "            best_settings = {'epochs': epochs, 'weight_decay': wd, 'learning_rate_poss': lr}\n",
    "    if verbose:\n",
    "        print('min loss %.3f' % min_loss)\n",
    "        print('best settings are', best_settings)\n",
    "    return min_loss, best_settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[      iid  gender    pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       " 0       1       0   11.0   27.0      35.0      20.0      20.0      20.0   \n",
       " 1       1       0   12.0   22.0      60.0       0.0       0.0      40.0   \n",
       " 2       1       0   13.0   22.0      19.0      18.0      19.0      18.0   \n",
       " 3       1       0   14.0   23.0      30.0       5.0      15.0      40.0   \n",
       " 4       1       0   15.0   24.0      30.0      10.0      20.0      10.0   \n",
       " ...   ...     ...    ...    ...       ...       ...       ...       ...   \n",
       " 8373  552       1  526.0   26.0      10.0      10.0      30.0      20.0   \n",
       " 8374  552       1  527.0   24.0      50.0      20.0      10.0       5.0   \n",
       " 8375  552       1  528.0   29.0      40.0      10.0      30.0      10.0   \n",
       " 8376  552       1  529.0   22.0      10.0      25.0      25.0      10.0   \n",
       " 8377  552       1  530.0   22.0      20.0      20.0      10.0      15.0   \n",
       " \n",
       "       pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
       " 0          0.0   5.00000  ...         0         0         1           1   \n",
       " 1          0.0   0.00000  ...         0         0         1           1   \n",
       " 2         14.0  12.00000  ...         0         0         1           1   \n",
       " 3          5.0   5.00000  ...         0         0         1           1   \n",
       " 4         10.0  20.00000  ...         0         0         1           1   \n",
       " ...        ...       ...  ...       ...       ...       ...         ...   \n",
       " 8373      10.0  15.00000  ...         0         0         0           1   \n",
       " 8374      10.0   5.00000  ...         0         0         0           1   \n",
       " 8375      10.0  11.84593  ...         0         0         0           1   \n",
       " 8376      10.0  20.00000  ...         0         0         0           1   \n",
       " 8377       5.0  30.00000  ...         0         0         0           1   \n",
       " \n",
       "       go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
       " 0              0           0           0           0           0           0  \n",
       " 1              0           0           0           0           0           0  \n",
       " 2              0           0           0           0           0           0  \n",
       " 3              0           0           0           0           0           0  \n",
       " 4              0           0           0           0           0           0  \n",
       " ...          ...         ...         ...         ...         ...         ...  \n",
       " 8373           0           0           0           0           0           0  \n",
       " 8374           0           0           0           0           0           0  \n",
       " 8375           0           0           0           0           0           0  \n",
       " 8376           0           0           0           0           0           0  \n",
       " 8377           0           0           0           0           0           0  \n",
       " \n",
       " [8378 rows x 110 columns],\n",
       "       iid  gender    pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       " 0       1       0   11.0   27.0      35.0      20.0      20.0      20.0   \n",
       " 1       1       0   12.0   22.0      60.0       0.0       0.0      40.0   \n",
       " 2       1       0   13.0   22.0      19.0      18.0      19.0      18.0   \n",
       " 3       1       0   14.0   23.0      30.0       5.0      15.0      40.0   \n",
       " 4       1       0   15.0   24.0      30.0      10.0      20.0      10.0   \n",
       " ...   ...     ...    ...    ...       ...       ...       ...       ...   \n",
       " 8373  552       1  526.0   26.0      10.0      10.0      30.0      20.0   \n",
       " 8374  552       1  527.0   24.0      50.0      20.0      10.0       5.0   \n",
       " 8375  552       1  528.0   29.0      40.0      10.0      30.0      10.0   \n",
       " 8376  552       1  529.0   22.0      10.0      25.0      25.0      10.0   \n",
       " 8377  552       1  530.0   22.0      20.0      20.0      10.0      15.0   \n",
       " \n",
       "       pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
       " 0          0.0      5.00  ...         0         0         1           1   \n",
       " 1          0.0      0.00  ...         0         0         1           1   \n",
       " 2         14.0     12.00  ...         0         0         1           1   \n",
       " 3          5.0      5.00  ...         0         0         1           1   \n",
       " 4         10.0     20.00  ...         0         0         1           1   \n",
       " ...        ...       ...  ...       ...       ...       ...         ...   \n",
       " 8373      10.0     15.00  ...         0         0         0           1   \n",
       " 8374      10.0      5.00  ...         0         0         0           1   \n",
       " 8375      10.0     10.64  ...         0         0         0           1   \n",
       " 8376      10.0     20.00  ...         0         0         0           1   \n",
       " 8377       5.0     30.00  ...         0         0         0           1   \n",
       " \n",
       "       go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
       " 0              0           0           0           0           0           0  \n",
       " 1              0           0           0           0           0           0  \n",
       " 2              0           0           0           0           0           0  \n",
       " 3              0           0           0           0           0           0  \n",
       " 4              0           0           0           0           0           0  \n",
       " ...          ...         ...         ...         ...         ...         ...  \n",
       " 8373           0           0           0           0           0           0  \n",
       " 8374           0           0           0           0           0           0  \n",
       " 8375           0           0           0           0           0           0  \n",
       " 8376           0           0           0           0           0           0  \n",
       " 8377           0           0           0           0           0           0  \n",
       " \n",
       " [8378 rows x 110 columns],\n",
       "       iid  gender    pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       " 0       1       0   11.0   27.0      35.0      20.0      20.0      20.0   \n",
       " 1       1       0   12.0   22.0      60.0       0.0       0.0      40.0   \n",
       " 2       1       0   13.0   22.0      19.0      18.0      19.0      18.0   \n",
       " 3       1       0   14.0   23.0      30.0       5.0      15.0      40.0   \n",
       " 4       1       0   15.0   24.0      30.0      10.0      20.0      10.0   \n",
       " ...   ...     ...    ...    ...       ...       ...       ...       ...   \n",
       " 8373  552       1  526.0   26.0      10.0      10.0      30.0      20.0   \n",
       " 8374  552       1  527.0   24.0      50.0      20.0      10.0       5.0   \n",
       " 8375  552       1  528.0   29.0      40.0      10.0      30.0      10.0   \n",
       " 8376  552       1  529.0   22.0      10.0      25.0      25.0      10.0   \n",
       " 8377  552       1  530.0   22.0      20.0      20.0      10.0      15.0   \n",
       " \n",
       "       pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
       " 0          0.0   5.00000  ...         0         0         1           1   \n",
       " 1          0.0   0.00000  ...         0         0         1           1   \n",
       " 2         14.0  12.00000  ...         0         0         1           1   \n",
       " 3          5.0   5.00000  ...         0         0         1           1   \n",
       " 4         10.0  20.00000  ...         0         0         1           1   \n",
       " ...        ...       ...  ...       ...       ...       ...         ...   \n",
       " 8373      10.0  15.00000  ...         0         0         0           1   \n",
       " 8374      10.0   5.00000  ...         0         0         0           1   \n",
       " 8375      10.0  11.84593  ...         0         0         0           1   \n",
       " 8376      10.0  20.00000  ...         0         0         0           1   \n",
       " 8377       5.0  30.00000  ...         0         0         0           1   \n",
       " \n",
       "       go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
       " 0              0           0           0           0           0           0  \n",
       " 1              0           0           0           0           0           0  \n",
       " 2              0           0           0           0           0           0  \n",
       " 3              0           0           0           0           0           0  \n",
       " 4              0           0           0           0           0           0  \n",
       " ...          ...         ...         ...         ...         ...         ...  \n",
       " 8373           0           0           0           0           0           0  \n",
       " 8374           0           0           0           0           0           0  \n",
       " 8375           0           0           0           0           0           0  \n",
       " 8376           0           0           0           0           0           0  \n",
       " 8377           0           0           0           0           0           0  \n",
       " \n",
       " [8378 rows x 105 columns],\n",
       "       iid  gender    pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       " 0       1       0   11.0   27.0      35.0      20.0      20.0      20.0   \n",
       " 1       1       0   12.0   22.0      60.0       0.0       0.0      40.0   \n",
       " 2       1       0   13.0   22.0      19.0      18.0      19.0      18.0   \n",
       " 3       1       0   14.0   23.0      30.0       5.0      15.0      40.0   \n",
       " 4       1       0   15.0   24.0      30.0      10.0      20.0      10.0   \n",
       " ...   ...     ...    ...    ...       ...       ...       ...       ...   \n",
       " 8373  552       1  526.0   26.0      10.0      10.0      30.0      20.0   \n",
       " 8374  552       1  527.0   24.0      50.0      20.0      10.0       5.0   \n",
       " 8375  552       1  528.0   29.0      40.0      10.0      30.0      10.0   \n",
       " 8376  552       1  529.0   22.0      10.0      25.0      25.0      10.0   \n",
       " 8377  552       1  530.0   22.0      20.0      20.0      10.0      15.0   \n",
       " \n",
       "       pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
       " 0          0.0      5.00  ...         0         0         1           1   \n",
       " 1          0.0      0.00  ...         0         0         1           1   \n",
       " 2         14.0     12.00  ...         0         0         1           1   \n",
       " 3          5.0      5.00  ...         0         0         1           1   \n",
       " 4         10.0     20.00  ...         0         0         1           1   \n",
       " ...        ...       ...  ...       ...       ...       ...         ...   \n",
       " 8373      10.0     15.00  ...         0         0         0           1   \n",
       " 8374      10.0      5.00  ...         0         0         0           1   \n",
       " 8375      10.0     10.64  ...         0         0         0           1   \n",
       " 8376      10.0     20.00  ...         0         0         0           1   \n",
       " 8377       5.0     30.00  ...         0         0         0           1   \n",
       " \n",
       "       go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
       " 0              0           0           0           0           0           0  \n",
       " 1              0           0           0           0           0           0  \n",
       " 2              0           0           0           0           0           0  \n",
       " 3              0           0           0           0           0           0  \n",
       " 4              0           0           0           0           0           0  \n",
       " ...          ...         ...         ...         ...         ...         ...  \n",
       " 8373           0           0           0           0           0           0  \n",
       " 8374           0           0           0           0           0           0  \n",
       " 8375           0           0           0           0           0           0  \n",
       " 8376           0           0           0           0           0           0  \n",
       " 8377           0           0           0           0           0           0  \n",
       " \n",
       " [8378 rows x 105 columns],\n",
       "       iid  gender    pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       " 0       1       0   11.0   27.0      35.0      20.0      20.0      20.0   \n",
       " 1       1       0   12.0   22.0      60.0       0.0       0.0      40.0   \n",
       " 2       1       0   13.0   22.0      19.0      18.0      19.0      18.0   \n",
       " 3       1       0   14.0   23.0      30.0       5.0      15.0      40.0   \n",
       " 4       1       0   15.0   24.0      30.0      10.0      20.0      10.0   \n",
       " ...   ...     ...    ...    ...       ...       ...       ...       ...   \n",
       " 8373  552       1  526.0   26.0      10.0      10.0      30.0      20.0   \n",
       " 8374  552       1  527.0   24.0      50.0      20.0      10.0       5.0   \n",
       " 8375  552       1  528.0   29.0      40.0      10.0      30.0      10.0   \n",
       " 8376  552       1  529.0   22.0      10.0      25.0      25.0      10.0   \n",
       " 8377  552       1  530.0   22.0      20.0      20.0      10.0      15.0   \n",
       " \n",
       "       pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
       " 0          0.0   5.00000  ...         0         0         1           1   \n",
       " 1          0.0   0.00000  ...         0         0         1           1   \n",
       " 2         14.0  12.00000  ...         0         0         1           1   \n",
       " 3          5.0   5.00000  ...         0         0         1           1   \n",
       " 4         10.0  20.00000  ...         0         0         1           1   \n",
       " ...        ...       ...  ...       ...       ...       ...         ...   \n",
       " 8373      10.0  15.00000  ...         0         0         0           1   \n",
       " 8374      10.0   5.00000  ...         0         0         0           1   \n",
       " 8375      10.0  11.84593  ...         0         0         0           1   \n",
       " 8376      10.0  20.00000  ...         0         0         0           1   \n",
       " 8377       5.0  30.00000  ...         0         0         0           1   \n",
       " \n",
       "       go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
       " 0              0           0           0           0           0           0  \n",
       " 1              0           0           0           0           0           0  \n",
       " 2              0           0           0           0           0           0  \n",
       " 3              0           0           0           0           0           0  \n",
       " 4              0           0           0           0           0           0  \n",
       " ...          ...         ...         ...         ...         ...         ...  \n",
       " 8373           0           0           0           0           0           0  \n",
       " 8374           0           0           0           0           0           0  \n",
       " 8375           0           0           0           0           0           0  \n",
       " 8376           0           0           0           0           0           0  \n",
       " 8377           0           0           0           0           0           0  \n",
       " \n",
       " [8378 rows x 109 columns],\n",
       "       iid  gender    pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       " 0       1       0   11.0   27.0      35.0      20.0      20.0      20.0   \n",
       " 1       1       0   12.0   22.0      60.0       0.0       0.0      40.0   \n",
       " 2       1       0   13.0   22.0      19.0      18.0      19.0      18.0   \n",
       " 3       1       0   14.0   23.0      30.0       5.0      15.0      40.0   \n",
       " 4       1       0   15.0   24.0      30.0      10.0      20.0      10.0   \n",
       " ...   ...     ...    ...    ...       ...       ...       ...       ...   \n",
       " 8373  552       1  526.0   26.0      10.0      10.0      30.0      20.0   \n",
       " 8374  552       1  527.0   24.0      50.0      20.0      10.0       5.0   \n",
       " 8375  552       1  528.0   29.0      40.0      10.0      30.0      10.0   \n",
       " 8376  552       1  529.0   22.0      10.0      25.0      25.0      10.0   \n",
       " 8377  552       1  530.0   22.0      20.0      20.0      10.0      15.0   \n",
       " \n",
       "       pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
       " 0          0.0      5.00  ...         0         0         1           1   \n",
       " 1          0.0      0.00  ...         0         0         1           1   \n",
       " 2         14.0     12.00  ...         0         0         1           1   \n",
       " 3          5.0      5.00  ...         0         0         1           1   \n",
       " 4         10.0     20.00  ...         0         0         1           1   \n",
       " ...        ...       ...  ...       ...       ...       ...         ...   \n",
       " 8373      10.0     15.00  ...         0         0         0           1   \n",
       " 8374      10.0      5.00  ...         0         0         0           1   \n",
       " 8375      10.0     10.64  ...         0         0         0           1   \n",
       " 8376      10.0     20.00  ...         0         0         0           1   \n",
       " 8377       5.0     30.00  ...         0         0         0           1   \n",
       " \n",
       "       go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
       " 0              0           0           0           0           0           0  \n",
       " 1              0           0           0           0           0           0  \n",
       " 2              0           0           0           0           0           0  \n",
       " 3              0           0           0           0           0           0  \n",
       " 4              0           0           0           0           0           0  \n",
       " ...          ...         ...         ...         ...         ...         ...  \n",
       " 8373           0           0           0           0           0           0  \n",
       " 8374           0           0           0           0           0           0  \n",
       " 8375           0           0           0           0           0           0  \n",
       " 8376           0           0           0           0           0           0  \n",
       " 8377           0           0           0           0           0           0  \n",
       " \n",
       " [8378 rows x 109 columns],\n",
       "       iid  gender    pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       " 0       1       0   11.0   27.0      35.0      20.0      20.0      20.0   \n",
       " 1       1       0   12.0   22.0      60.0       0.0       0.0      40.0   \n",
       " 2       1       0   13.0   22.0      19.0      18.0      19.0      18.0   \n",
       " 3       1       0   14.0   23.0      30.0       5.0      15.0      40.0   \n",
       " 4       1       0   15.0   24.0      30.0      10.0      20.0      10.0   \n",
       " ...   ...     ...    ...    ...       ...       ...       ...       ...   \n",
       " 8373  552       1  526.0   26.0      10.0      10.0      30.0      20.0   \n",
       " 8374  552       1  527.0   24.0      50.0      20.0      10.0       5.0   \n",
       " 8375  552       1  528.0   29.0      40.0      10.0      30.0      10.0   \n",
       " 8376  552       1  529.0   22.0      10.0      25.0      25.0      10.0   \n",
       " 8377  552       1  530.0   22.0      20.0      20.0      10.0      15.0   \n",
       " \n",
       "       pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
       " 0          0.0       5.0  ...         0         0         1           1   \n",
       " 1          0.0       0.0  ...         0         0         1           1   \n",
       " 2         14.0      12.0  ...         0         0         1           1   \n",
       " 3          5.0       5.0  ...         0         0         1           1   \n",
       " 4         10.0      20.0  ...         0         0         1           1   \n",
       " ...        ...       ...  ...       ...       ...       ...         ...   \n",
       " 8373      10.0      15.0  ...         0         0         0           1   \n",
       " 8374      10.0       5.0  ...         0         0         0           1   \n",
       " 8375      10.0       NaN  ...         0         0         0           1   \n",
       " 8376      10.0      20.0  ...         0         0         0           1   \n",
       " 8377       5.0      30.0  ...         0         0         0           1   \n",
       " \n",
       "       go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
       " 0              0           0           0           0           0           0  \n",
       " 1              0           0           0           0           0           0  \n",
       " 2              0           0           0           0           0           0  \n",
       " 3              0           0           0           0           0           0  \n",
       " 4              0           0           0           0           0           0  \n",
       " ...          ...         ...         ...         ...         ...         ...  \n",
       " 8373           0           0           0           0           0           0  \n",
       " 8374           0           0           0           0           0           0  \n",
       " 8375           0           0           0           0           0           0  \n",
       " 8376           0           0           0           0           0           0  \n",
       " 8377           0           0           0           0           0           0  \n",
       " \n",
       " [8378 rows x 104 columns]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_sample defines size of train set\n",
    "train_sample = 0.8\n",
    "# our output is only one number\n",
    "output_dim = 1\n",
    "\n",
    "for dataset in poss_LG_data_sets:\n",
    "    cross_valid_regression()\n",
    "\n",
    "# #simple division into training and testing sets\n",
    "# msk = np.random.rand(len(regression_df_1)) < train_sample\n",
    "# train_set = regression_df_1[msk]\n",
    "# test_set = regression_df_1[~msk]\n",
    "\n",
    "# #we want to predict 'dec' based on other attributes. We dont want 'iid' and 'pid' columns\n",
    "# #to have any impact on results\n",
    "# train_x = train_set.drop(['dec', 'iid', 'pid', 'gender'], axis=1).values\n",
    "# train_y = train_set['dec'].values \n",
    "\n",
    "# test_x = test_set.drop(['dec', 'iid', 'pid'], axis=1).values\n",
    "# test_y = test_set['dec'].values \n",
    "\n",
    "# # input_dim = train_x.shape[1]\n",
    "# # model = LogisticRegression(input_dim, output_dim).to(dev)\n",
    "# # print(cross_valid_regression(model, train_x, train_y, test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "men_like_women_df:\n",
      "       id  gender  pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
      "100     0     1.0    0   21.0      15.0      20.0      20.0      15.0   \n",
      "101     0     1.0    1   24.0      45.0       5.0      25.0      20.0   \n",
      "102     0     1.0    2   25.0      35.0      10.0      35.0      10.0   \n",
      "103     0     1.0    3   23.0      20.0      20.0      20.0      20.0   \n",
      "104     0     1.0    4   21.0      20.0       5.0      25.0      25.0   \n",
      "...   ...     ...  ...    ...       ...       ...       ...       ...   \n",
      "8373  276     1.0  270   26.0      10.0      10.0      30.0      20.0   \n",
      "8374  276     1.0  271   24.0      50.0      20.0      10.0       5.0   \n",
      "8375  276     1.0  272   29.0      40.0      10.0      30.0      10.0   \n",
      "8376  276     1.0  273   22.0      10.0      25.0      25.0      10.0   \n",
      "8377  276     1.0  274   22.0      20.0      20.0      10.0      15.0   \n",
      "\n",
      "      pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
      "100       15.0  15.00000  ...       1.0       0.0       0.0         0.0   \n",
      "101        0.0   5.00000  ...       1.0       0.0       0.0         0.0   \n",
      "102       10.0   0.00000  ...       1.0       0.0       0.0         0.0   \n",
      "103       10.0  10.00000  ...       1.0       0.0       0.0         0.0   \n",
      "104       10.0  15.00000  ...       1.0       0.0       0.0         0.0   \n",
      "...        ...       ...  ...       ...       ...       ...         ...   \n",
      "8373      10.0  15.00000  ...       0.0       0.0       0.0         1.0   \n",
      "8374      10.0   5.00000  ...       0.0       0.0       0.0         1.0   \n",
      "8375      10.0  11.84593  ...       0.0       0.0       0.0         1.0   \n",
      "8376      10.0  20.00000  ...       0.0       0.0       0.0         1.0   \n",
      "8377       5.0  30.00000  ...       0.0       0.0       0.0         1.0   \n",
      "\n",
      "      go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
      "100          0.0         0.0         1.0         0.0         0.0         0.0  \n",
      "101          0.0         0.0         1.0         0.0         0.0         0.0  \n",
      "102          0.0         0.0         1.0         0.0         0.0         0.0  \n",
      "103          0.0         0.0         1.0         0.0         0.0         0.0  \n",
      "104          0.0         0.0         1.0         0.0         0.0         0.0  \n",
      "...          ...         ...         ...         ...         ...         ...  \n",
      "8373         0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "8374         0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "8375         0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "8376         0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "8377         0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "\n",
      "[4194 rows x 109 columns]\n",
      "\n",
      "women_like_men_df:\n",
      "       id  gender  pid  age_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
      "0       0     0.0    0   27.0      35.0      20.0      20.0      20.0   \n",
      "1       0     0.0    1   22.0      60.0       0.0       0.0      40.0   \n",
      "2       0     0.0    2   22.0      19.0      18.0      19.0      18.0   \n",
      "3       0     0.0    3   23.0      30.0       5.0      15.0      40.0   \n",
      "4       0     0.0    4   24.0      30.0      10.0      20.0      10.0   \n",
      "...   ...     ...  ...    ...       ...       ...       ...       ...   \n",
      "7889  273     0.0  272   30.0      40.0      10.0      20.0      10.0   \n",
      "7890  273     0.0  273   28.0      20.0      20.0      20.0      20.0   \n",
      "7891  273     0.0  274   30.0      30.0       3.0      30.0      30.0   \n",
      "7892  273     0.0  275   27.0      40.0      20.0      20.0      20.0   \n",
      "7893  273     0.0  276   25.0      70.0       0.0      15.0      15.0   \n",
      "\n",
      "      pf_o_amb  pf_o_sha  ...  date_5.0  date_6.0  date_7.0  go_out_1.0  \\\n",
      "0          0.0       5.0  ...       0.0       0.0       1.0         1.0   \n",
      "1          0.0       0.0  ...       0.0       0.0       1.0         1.0   \n",
      "2         14.0      12.0  ...       0.0       0.0       1.0         1.0   \n",
      "3          5.0       5.0  ...       0.0       0.0       1.0         1.0   \n",
      "4         10.0      20.0  ...       0.0       0.0       1.0         1.0   \n",
      "...        ...       ...  ...       ...       ...       ...         ...   \n",
      "7889       0.0      20.0  ...       0.0       0.0       1.0         0.0   \n",
      "7890       0.0      20.0  ...       0.0       0.0       1.0         0.0   \n",
      "7891       3.0       4.0  ...       0.0       0.0       1.0         0.0   \n",
      "7892       0.0       0.0  ...       0.0       0.0       1.0         0.0   \n",
      "7893       0.0       0.0  ...       0.0       0.0       1.0         0.0   \n",
      "\n",
      "      go_out_2.0  go_out_3.0  go_out_4.0  go_out_5.0  go_out_6.0  go_out_7.0  \n",
      "0            0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "1            0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "2            0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "3            0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "4            0.0         0.0         0.0         0.0         0.0         0.0  \n",
      "...          ...         ...         ...         ...         ...         ...  \n",
      "7889         0.0         1.0         0.0         0.0         0.0         0.0  \n",
      "7890         0.0         1.0         0.0         0.0         0.0         0.0  \n",
      "7891         0.0         1.0         0.0         0.0         0.0         0.0  \n",
      "7892         0.0         1.0         0.0         0.0         0.0         0.0  \n",
      "7893         0.0         1.0         0.0         0.0         0.0         0.0  \n",
      "\n",
      "[4184 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "women_like_men_CV_df = base_df.copy()\n",
    "women_like_men_CV_df.drop(columns=['income'], inplace=True)\n",
    "women_like_men_CV_df = fill_with_mean(women_like_men_CV_df)\n",
    "women_like_men_CV_df.rename(columns={'iid':'id'},inplace=True)\n",
    "men_like_women_data_dates = []\n",
    "women_like_men_data_dates = []\n",
    "\n",
    "for _, row in women_like_men_CV_df.iterrows():\n",
    "    if row['gender']:\n",
    "        # it's a woman\n",
    "        men_like_women_data_dates.append(row)\n",
    "    else:\n",
    "        women_like_men_data_dates.append(row)\n",
    "        \n",
    "men_like_women_df_dates = encode_data(pd.DataFrame(men_like_women_data_dates))\n",
    "women_like_men_df_dates = encode_data(pd.DataFrame(women_like_men_data_dates))\n",
    "\n",
    "nr_of_dates = len(men_like_women_df_dates)\n",
    "\n",
    "print(\"men_like_women_df:\")\n",
    "print(men_like_women_df_dates)\n",
    "print(\"\\nwomen_like_men_df:\")\n",
    "print(women_like_men_df_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because of the data set being fairly small, we are going to be doing a Leave One Out Cross Validation to get parameters for the Matrix Factorization model training on new date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To first train MF and LR models we use parameters computed by cross validation before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_k_out(MF_df, df_dates_user, df_dates_item, user_id):\n",
    "    ###Extracts users whith user_id from user_id and then for every user extracts one item for it\n",
    "    ###----------------------------------------Extracting users-----------------------###\n",
    "    ###Leave one user(of user_id) and every item we have his decision on out of the MF_df\n",
    "    train_data_no_users = []\n",
    "    test_data = []\n",
    "    \n",
    "    for _,row in MF_df.iterrows():\n",
    "        if row[\"id\"] in user_id:\n",
    "            test_data.append(row)\n",
    "        else:\n",
    "            train_data_no_users.append(row)\n",
    "    \n",
    "    ###Leave one user(of user_id) and every item we have his decision on out of the LR_df\n",
    "    train_dates_user = []\n",
    "    test_dates = []\n",
    "    \n",
    "    for _,row in df_dates_user.iterrows():\n",
    "        if row[\"id\"] in user_id:\n",
    "            test_dates.append(row)\n",
    "        else:\n",
    "            train_dates_user.append(row)\n",
    "    \n",
    "    train_dates_items = []\n",
    "    \n",
    "    for _,row in df_dates_item.iterrows():\n",
    "        if row['pid'] not in user_id:\n",
    "            train_dates_items.append(row)\n",
    "            \n",
    "    ###------------------------------------Pair up users with items---------------###\n",
    "    ###Making test data and set have only one exclusive pair of (user,item)\n",
    "    item_id = []\n",
    "    test_data_final = []\n",
    "    \n",
    "    for user in user_id:\n",
    "        for row in test_data:\n",
    "            if row[\"id\"] == user:\n",
    "                item_id.append(row[\"pid\"])\n",
    "                test_data_final.append(row)\n",
    "                break\n",
    "            \n",
    "    \n",
    "    test_dates_final = []\n",
    "    for dec_row in test_data_final:\n",
    "        user,item = dec_row[['id','pid']]\n",
    "        for row in test_dates:\n",
    "            if row['id'] == user and row['pid'] == item:\n",
    "                test_dates_final.append(row)\n",
    "                break\n",
    "        \n",
    "    ###--------------------------------------Extracting items-------------------------###\n",
    "    ### Training dataframe for base MF without user(of user_id), and his decisions upon which we can test the results.\n",
    "\n",
    "    train_data_final = []\n",
    "    for row in train_data_no_users:\n",
    "        if row[\"pid\"] not in item_id:\n",
    "            train_data_final.append(row)\n",
    "    \n",
    "    train_dates_user_final= []\n",
    "    for row in train_dates_user:\n",
    "        if row[\"pid\"] not in item_id:\n",
    "            train_dates_user_final.append(row)\n",
    "    \n",
    "    train_dates_items_final= []\n",
    "    for row in train_dates_items:\n",
    "        if row[\"id\"] not in item_id:\n",
    "            train_dates_items_final.append(row)\n",
    "    \n",
    "    \n",
    "    train_df = pd.DataFrame(train_data_final)\n",
    "    \n",
    "    ### Training dataframe for base LR without user(of user_id), and his dates upon which we can predict his decision\n",
    "    ###     and test them against test_df.\n",
    "    train_dates_user_final = pd.DataFrame(train_dates_user_final)\n",
    "    train_dates_items_final = pd.DataFrame(train_dates_items_final)\n",
    "    return train_df, test_data_final, train_dates_user_final, train_dates_items_final, test_dates_final\n",
    "\n",
    "def new_date_test(MF_model, dec_Matrix, df_dates_men, df_dates_women, new_date, nr_men, nr_women, dec,sim_fun = get_similarity,\n",
    "                 epochs=100, learning_rate=0.001, weight_decay=0.001,K=0.1,P=0.1):\n",
    "    \n",
    "    Y = torch.FloatTensor([dec]).to(dev)\n",
    "    y_hat = get_new_date_dec(MF_model, dec_Matrix, df_dates_men, df_dates_women, new_date,nr_men= nr_men,nr_women= nr_women,\n",
    "                             epochs=epochs, learning_rate=learning_rate, weight_decay=weight_decay,sim_fun =sim_fun,K=K,P=P,CV_mode=True)\n",
    "    \n",
    "    loss = F.mse_loss(y_hat, Y)\n",
    "    return loss.item()\n",
    "\n",
    "# epochs_poss = list(range(60, 350, 10))\n",
    "# weight_decay_poss = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# learning_rate_poss = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "K_poss = [0.05,0.1]\n",
    "P_poss = [0.05,0.1]\n",
    "sim_funs = [get_similarity, get_similarity_cosine]\n",
    "\n",
    "\n",
    "### Cross valid for women_like_men\n",
    "def KFoldCrossValidation(MF_df, df_dates_user , df_dates_item, input_dim = 105, output_dim = 1,K = 10,verbose=False):\n",
    "    nr_users= 274\n",
    "    nr_items= 277\n",
    "    \n",
    "    min_overall_loss = np.inf\n",
    "    best_overall_settings = None\n",
    "    skip_first = True\n",
    "    \n",
    "    for (N, P, fun) in tqdm(product(K_poss, P_poss, sim_funs)):\n",
    "        if skip_first:\n",
    "            skip_first = False\n",
    "            continue\n",
    "        nr_users= 274\n",
    "        nr_items= 277\n",
    "        \n",
    "        min_user_loss = np.inf\n",
    "        best_settings = None\n",
    "        \n",
    "        indices = [idx for idx in range(nr_users)]\n",
    "        np.random.shuffle(indices)\n",
    "        user_folds = np.array_split(indices,K)\n",
    "        \n",
    "        \n",
    "        for user_batch in tqdm(user_folds):\n",
    "            \n",
    "            MF_train_df, dec_test_df, train_dates_user,train_dates_items, dates_test_df = leave_k_out(MF_df, df_dates_user, df_dates_item, user_batch)\n",
    "            MF_train_df.rename(columns={'dec':'decision'},inplace=True)\n",
    "            MF_train_df = encode_data(MF_train_df)\n",
    "            nr_users = len(MF_train_df.id.unique())\n",
    "            nr_items = len(MF_train_df.pid.unique())\n",
    "            \n",
    "            train_dates_user = encode_data(train_dates_user)\n",
    "            train_dates_items = encode_data(train_dates_items)\n",
    "\n",
    "            MF_model = MatrixFactorization(nr_users, nr_items, weights=(0, 1), bias=(-1,1), emb_size=2000).to(dev)            \n",
    "            \n",
    "            MF_train(MF_model, MF_train_df, epochs = 1400, learning_rate = 0.001, weight_decay= 1e-6) ## Change train function\n",
    "\n",
    "            MF_model.eval()\n",
    "            \n",
    "            fold_loss = 0.    \n",
    "\n",
    "            dec_Matrix = np.array([[get_mf_dec(MF_model,user_id,item_id) for user_id in range(nr_users)]\n",
    "                                   for item_id in range(nr_items)])\n",
    "            \n",
    "            for dec_row, new_date_row in zip(dec_test_df,dates_test_df):\n",
    "                fold_loss += new_date_test(MF_model, dec_Matrix, train_dates_items, train_dates_user, new_date_row,\n",
    "                                         nr_items, nr_users, dec_row[\"decision\"],sim_fun = fun,\n",
    "                                          epochs=1400, learning_rate=0.001, weight_decay=1e-6,K=N,P=P)\n",
    "            fold_loss /= len(user_batch)\n",
    "            \n",
    "            if (fold_loss < min_user_loss) or (fold_loss == min_user_loss and best_user_settings is not None):\n",
    "                min_user_loss = fold_loss\n",
    "                best_user_settings = {'% of similar': N, '% of decisions': P, 'sim_function': fun.__name__}\n",
    "                \n",
    "        if (min_user_loss < min_overall_loss) or (min_user_loss == min_overall_loss and best_overall_settings is not None):\n",
    "            min_overall_loss = min_user_loss\n",
    "            best_overall_settings = best_user_settings\n",
    "            \n",
    "        if verbose:\n",
    "            print('min loss %.3f' % min_overall_loss)\n",
    "            print('best settings are', best_overall_settings)\n",
    "        \n",
    "    return min_overall_loss, best_overall_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                        | 1/5 [07:16<29:05, 436.31s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                          | 2/5 [14:25<21:37, 432.35s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 3/5 [21:41<14:27, 433.65s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/5 [28:51<07:12, 432.50s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [35:59<00:00, 431.83s/it]\u001b[A\n",
      "2it [35:59, 1079.59s/it]\n",
      "  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss 0.479\n",
      "best settings are {'% of similar': 0.05, '% of decisions': 0.05, 'sim_function': 'get_similarity_cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                        | 1/5 [07:54<31:36, 474.16s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                          | 2/5 [15:34<23:17, 465.94s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 3/5 [22:47<15:02, 451.14s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/5 [29:48<07:19, 439.11s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [36:41<00:00, 440.25s/it]\u001b[A\n",
      "3it [1:12:40, 1546.94s/it]\n",
      "  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss 0.468\n",
      "best settings are {'% of similar': 0.05, '% of decisions': 0.1, 'sim_function': 'get_similarity'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                        | 1/5 [06:28<25:55, 388.87s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                          | 2/5 [12:58<19:27, 389.17s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 3/5 [19:23<12:54, 387.37s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/5 [25:53<06:28, 388.39s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [32:17<00:00, 387.40s/it]\u001b[A\n",
      "4it [1:44:57, 1692.49s/it]\n",
      "  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss 0.463\n",
      "best settings are {'% of similar': 0.05, '% of decisions': 0.1, 'sim_function': 'get_similarity_cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                        | 1/5 [06:54<27:36, 414.18s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                          | 2/5 [13:42<20:32, 410.75s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 3/5 [20:39<13:46, 413.47s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/5 [27:42<06:57, 417.30s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [34:31<00:00, 414.26s/it]\u001b[A\n",
      "5it [2:19:28, 1824.20s/it]\n",
      "  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss 0.463\n",
      "best settings are {'% of similar': 0.05, '% of decisions': 0.1, 'sim_function': 'get_similarity_cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                        | 1/5 [06:28<25:53, 388.29s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                          | 2/5 [12:53<19:19, 386.47s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 3/5 [19:23<12:56, 388.06s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/5 [25:44<06:25, 385.46s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [32:06<00:00, 385.40s/it]\u001b[A\n",
      "6it [2:51:35, 1858.31s/it]\n",
      "  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss 0.463\n",
      "best settings are {'% of similar': 0.05, '% of decisions': 0.1, 'sim_function': 'get_similarity_cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                        | 1/5 [06:56<27:46, 416.59s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                          | 2/5 [13:52<20:48, 416.22s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 3/5 [20:49<13:52, 416.48s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/5 [27:43<06:55, 415.63s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [34:28<00:00, 413.69s/it]\u001b[A\n",
      "7it [3:26:04, 1925.90s/it]\n",
      "  0%|                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss 0.463\n",
      "best settings are {'% of similar': 0.05, '% of decisions': 0.1, 'sim_function': 'get_similarity_cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                        | 1/5 [06:26<25:47, 386.96s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                          | 2/5 [12:56<19:26, 388.74s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 3/5 [19:25<12:57, 388.75s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/5 [25:50<06:27, 387.16s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [32:08<00:00, 385.71s/it]\u001b[A\n",
      "8it [3:58:12, 1786.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss 0.463\n",
      "best settings are {'% of similar': 0.05, '% of decisions': 0.1, 'sim_function': 'get_similarity_cosine'}\n",
      "Wall time: 3h 58min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_overall_loss, best_overall_settings = KFoldCrossValidation(women_like_men_df, women_like_men_df_dates, men_like_women_df_dates,K=5,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Fold Cross Validation on data set where we treat women as users and men as items:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gave best results ie. 46.3%  for :**\n",
    "* **Choosing similarity rate between users and items using Cosine Similarity between their according date attributes**\n",
    "* **Then computing average decisions for 5% of most similar users/items**\n",
    "* **Finally taking randomly selected 10% of the average decisions and using it as a MF baseline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Given two vectors x,y where $x_i$ gives user's x decision on partner i (pid = i), computes similiraty measure i.e. on how many positions does vector x match vector y, excluding \"-1\", the default value for no logistic regression output provided***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(x,y):\n",
    "    assert len(x) == len(y) , \"x should be same size as y\"\n",
    "    return pearsonr(torch.FloatTensor([x]).to(dev),torch.FloatTensor([y]).to(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_cosine(x,y):\n",
    "    assert len(x) == len(y) , \"x should be same size as y\"\n",
    "    return torch.nn.functional.cosine_similarity(torch.FloatTensor([x]).to(dev),torch.FloatTensor([y]).to(dev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Given matrix A(which has results only for one gender) where $a_{ij}$ represents output of logistic regression from a date between person(iid = i) and person(pid=j), creates vector C, where each position \"i\" represents similiarity between input vector person and other person \"i\" of the same gender***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(date_row):\n",
    "    subject_feats = [\"age\",\"race_4.0\",\"race_3.0\",\"race_2.0\",\"race_1.0\",\n",
    "                     \"race_o_4.0\",\"race_o_3.0\",\"race_o_2.0\",\"race_o_1.0\",\"imprace\",\"imprelig\",\"goal\",\"date_1.0\",\"date_2.0\",\n",
    "                     \"date_3.0\",\"date_4.0\",\"date_5.0\",\"date_6.0\",\"date_7.0\",\"go_out_7.0\",\"go_out_6.0\",\"go_out_5.0\",\"go_out_4.0\",\"go_out_3.0\",\"go_out_2.0\",\n",
    "                     \"go_out_1.0\",\"sports\",\"tvsports\",\"exercise\",\"dining\",\"museums\",\"art\",\"hiking\",\"gaming\",\"clubbing\",\"reading\",\"tv\",\n",
    "                     \"theater\",\"movies\",\"concerts\",\"music\",\"shopping\",\"yoga\",\"attr1_1\",\"sinc1_1\",\"intel1_1\",\"fun1_1\",\"amb1_1\",\"shar1_1\",\"attr2_1\",\n",
    "                     \"sinc2_1\",\"intel2_1\",\"fun2_1\",\"amb2_1\",\"shar2_1\",\"attr3_1\",\"sinc3_1\",\"fun3_1\",\"intel3_1\",\"amb3_1\",\"attr5_1\",\"sinc5_1\",\"intel5_1\",\n",
    "                     \"fun5_1\",\"amb5_1\",\"career_2.0\",\"career_3.0\",\"career_4.0\",\"career_5.0\",\"career_6.0\",\"career_7.0\",\"career_8.0\",\"career_9.0\",\"career_10.0\",\n",
    "                     \"career_11.0\",\"career_12.0\",\"career_13.0\",\"career_14.0\",\"career_15.0\",\"career_16.0\",\"career_17.0\",\"career_18.0\"]\n",
    "    \n",
    "    subject = date_row.drop([\"id\",\"pid\",\"gender\",'dec_o','dec',\"age_o\",\"pf_o_att\",\"pf_o_sin\",\"pf_o_int\",\"pf_o_fun\",\"pf_o_amb\",\"pf_o_sha\",\"attr_o\",\n",
    "                             \"sinc_o\",\"intel_o\",\"fun_o\",\"amb_o\",\"shar_o\",\"like_o\",\"prob_o\",\"met_o\",\"attr4_1\",\n",
    "                             \"sinc4_1\",\"intel4_1\",\"fun4_1\",\"amb4_1\",\"shar4_1\"])\n",
    "    \n",
    "    partner = date_row.drop(subject_feats + ['gender', 'id','dec_o', 'pid'])\n",
    "    return (date_row['id'],subject),(date_row['pid'],partner)\n",
    "\n",
    "def make_date(subject_info, partner_info):\n",
    "    return torch.FloatTensor([np.hstack((partner_info,subject_info))]).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mf_dec(mf_model, user_id, item_id):\n",
    "    return mf_model(torch.LongTensor([user_id]).to(dev),torch.LongTensor([item_id]).to(dev)).cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(MF_model, weights=(0,0.2),bias=(0,1), user_mode=False):\n",
    "    if user_mode:\n",
    "        ###Adding new row to user_emb\n",
    "        new_emb_row = torch.FloatTensor(1,MF_model.person_emb.embedding_dim).to(dev)\n",
    "        new_emb_row.data.uniform_(weights[0], weights[1])\n",
    "        MF_model.person_emb.weight = nn.Parameter(torch.cat((MF_model.person_emb.weight, new_emb_row))).to(dev)\n",
    "\n",
    "        ###Adding new bias row to user_bias\n",
    "        new_bias_row = torch.FloatTensor(1,MF_model.person_bias.embedding_dim).to(dev)\n",
    "        new_bias_row.data.uniform_(bias[0], bias[1])\n",
    "        MF_model.person_bias.weight = nn.Parameter(torch.cat((MF_model.person_bias.weight, new_bias_row))).to(dev)\n",
    "    else:\n",
    "        ###Adding new row to item_emb\n",
    "        new_emb_row = torch.FloatTensor(1,MF_model.partner_emb.embedding_dim).to(dev)\n",
    "        new_emb_row.data.uniform_(weights[0], weights[1])\n",
    "        MF_model.partner_emb.weight = nn.Parameter(torch.cat((MF_model.partner_emb.weight, new_emb_row))).to(dev)\n",
    "\n",
    "        ###Adding new bias row to item_bias\n",
    "        new_bias_row = torch.FloatTensor(1,MF_model.parnter_bias.embedding_dim).to(dev)\n",
    "        new_bias_row.data.uniform_(bias[0], bias[1])\n",
    "        MF_model.parnter_bias.weight = nn.Parameter(torch.cat((MF_model.parnter_bias.weight, new_bias_row))).to(dev)\n",
    "\n",
    "\n",
    "def train_one_row(MF_model, df_train,\n",
    "                  epochs=100, learning_rate=0.001, weight_decay=0.001,weights=(0,0.2),bias=(0,1),user_mode=False):\n",
    "    ###Creating a mask and registering a hook to zero out gradients of every embedding \n",
    "    ###    but the newly added ones, so that mf_model retrains only those.\n",
    "    item_mask_bias = torch.zeros_like(MF_model.parnter_bias.weight)\n",
    "    item_bias_hook = MF_model.parnter_bias.weight.register_hook(lambda grad: grad*item_mask_bias)\n",
    "    \n",
    "    user_mask_bias = torch.zeros_like(MF_model.person_bias.weight)\n",
    "    user_bias_hook = MF_model.person_bias.weight.register_hook(lambda grad: grad*user_mask_bias)\n",
    "    \n",
    "    item_mask_emb = torch.zeros_like(MF_model.partner_emb.weight)\n",
    "    item_emb_hook = MF_model.partner_emb.weight.register_hook(lambda grad: grad*item_mask_emb)\n",
    "    \n",
    "    user_mask_emb = torch.zeros_like(MF_model.person_emb.weight)\n",
    "    user_emb_hook = MF_model.person_emb.weight.register_hook(lambda grad: grad*user_mask_emb)\n",
    "    \n",
    "    if user_mode:\n",
    "        ### Setting to train only new_user's parameters\n",
    "        user_mask_emb[-1] = 1.\n",
    "        user_mask_bias[-1] = 1.\n",
    "    else:\n",
    "        ### Setting to train only new_item's parameters\n",
    "        item_mask_emb[-1] = 1.\n",
    "        item_mask_bias[-1] = 1.\n",
    "    \n",
    "    MF_model.train()\n",
    "    MF_train(MF_model,df_train,epochs=100, learning_rate=0.001, weight_decay=0.001)\n",
    "    MF_model.eval()\n",
    "    \n",
    "    ###Remove the hooks\n",
    "    item_bias_hook.remove()\n",
    "    item_emb_hook.remove()\n",
    "    user_bias_hook.remove()\n",
    "    user_emb_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_date_dec(mf_model,\n",
    "                     dec_Matrix,\n",
    "                     df_dates_men,\n",
    "                     df_dates_women,\n",
    "                     new_date = pd.Series,\n",
    "                     K = 0.1,\n",
    "                     P = 0.1,\n",
    "                     nr_men = 277, nr_women = 274,\n",
    "                     epochs=100, learning_rate=0.001, weight_decay=0.001,weights=(0,0.2),bias=(0,1),\n",
    "                     CV_mode = False,\n",
    "                     sim_fun = get_similarity\n",
    "                    ):\n",
    "    \n",
    "    MF_model = copy.deepcopy(mf_model).to(dev)\n",
    "    user,item = split_date(new_date)\n",
    "    ### Determine number of users and items depending on target's gender\n",
    "    if new_date[\"gender\"]: \n",
    "        #target is a man\n",
    "        nr_users = nr_men\n",
    "        nr_items = nr_women\n",
    "        df_user_dates = df_dates_men\n",
    "        df_item_dates = df_dates_women\n",
    "    else:\n",
    "        #target is a woman\n",
    "        nr_users = nr_women\n",
    "        nr_items = nr_men\n",
    "        df_user_dates = df_dates_women\n",
    "        df_item_dates = df_dates_men\n",
    "\n",
    "    \n",
    "\n",
    "    ###---------------------------------------------------Add new item----------------------------------------------------###\n",
    "    \n",
    "    ###First get every user's decision on the new item using logistic regression and -1 when user's date_info not provided\n",
    "    similarity_vec = np.array([-1.] * nr_items, dtype= np.float64)\n",
    "    for _,row in df_item_dates.iterrows():\n",
    "        _,_item = split_date(row)\n",
    "        similarity_rate = sim_fun(_item[1], item[1])\n",
    "        similarity_vec[int(_item[0])-1] = similarity_rate\n",
    "    \n",
    "    ### Get ids of K most similar items\n",
    "    most_sim_idx = np.argsort(similarity_vec)[:int(nr_items * K)]\n",
    "    \n",
    "    ###Compute the average decisions from K most similar items.\n",
    "    avg_dec_vec = np.mean(dec_Matrix[most_sim_idx],axis=0)\n",
    "    \n",
    "    ###Pick nr_users*P(percantage of users) of these decisions and use them as baseline decisions to retrain new item's decisions using MatrixFactorization\n",
    "    new_item_dec = np.array([-1.] * nr_users, dtype= np.float64)\n",
    "    random_idx = np.random.choice(range(nr_users),int(nr_users * P))\n",
    "    new_item_dec[random_idx] = avg_dec_vec[random_idx]\n",
    "   \n",
    "    ###Prepare dataframe for MF_model to train on\n",
    "    df_train = pd.DataFrame(data=[[user_id,nr_items,dec] for user_id,dec in enumerate(new_item_dec) if dec != -1] ,columns=[\"id\", \"pid\", \"decision\"])\n",
    "    \n",
    "    ###Update MF_model parameters to train only the new item's row\n",
    "    update_params(MF_model)\n",
    "    train_one_row(MF_model,df_train)\n",
    "\n",
    "    ###Add new item's decisions to the dec_Matrix\n",
    "    ###New item has been added.\n",
    "    nr_items += 1\n",
    "    \n",
    "    \n",
    "    ###---------------------------------------------------Add new user----------------------------------------------------###\n",
    "    \n",
    "    ###First get every new user's decisions on every item using logistic regression and -1 when item's date_info not provided\n",
    "    similarity_vec = np.array([-1.] * nr_users, dtype= np.float64)\n",
    "    for idx,row in df_user_dates.iterrows():\n",
    "        _user,_ = split_date(row)\n",
    "        similarity_rate = sim_fun(_user[1],user[1])\n",
    "        similarity_vec[int(_user[0])-1] = similarity_rate\n",
    "\n",
    "    ###Then get id's of most similar users to the new one, by comparing their decisions we got \n",
    "    ###    using matrix factorization and new items logistic regression ones.\n",
    "    most_sim_idx = np.argsort(similarity_vec)[:int(nr_items * K)]\n",
    "    \n",
    "    ###Compute the average decisions from K most similar users.\n",
    "    avg_dec_vec = np.mean([[item_dec[idx] for item_dec in dec_Matrix] for idx in most_sim_idx],axis=0)\n",
    "    \n",
    "    ###Take nr_items*P(percantage of items number) of these decisions and use them as a baseline for MF.\n",
    "    new_user_dec = np.array([-1.] * nr_items, dtype= np.float64)\n",
    "    random_idx = np.random.choice(range(nr_items-1),int(nr_items * P))\n",
    "    new_user_dec[random_idx] = avg_dec_vec[random_idx]\n",
    "    \n",
    "    ###Prepare dataframe for MF_model to train on\n",
    "    df_train = pd.DataFrame(data=[[nr_users,item_id,dec] for item_id, dec in enumerate(new_user_dec) if dec != -1],columns=[\"id\", \"pid\", \"decision\"])\n",
    "\n",
    "    ###Update MF_model parameters to train only the new user's row\n",
    "    update_params(MF_model,user_mode=True)\n",
    "    train_one_row(MF_model,df_train,user_mode=True)\n",
    "    \n",
    "    if CV_mode:\n",
    "        return MF_model(torch.LongTensor([nr_users]).to(dev),torch.LongTensor([nr_items-1]).to(dev))\n",
    "    \n",
    "    ###return new user's decision on new item\n",
    "    return get_mf_dec(MF_model, nr_users, nr_items-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
